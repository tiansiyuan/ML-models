{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff204b0-bed4-4636-93e2-640da4484aed",
   "metadata": {},
   "source": [
    "# OpenNMT\n",
    "\n",
    "The instructions and code are taken from [Neural Machine Translation (NMT) tutorial with OpenNMT-py](https://github.com/ymoslem/OpenNMT-Tutorial) by combining\n",
    "the two Jupyter notebooks together with a little bit tweaking.\n",
    "\n",
    "### Data Gathering and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e265f69-c509-4e0b-ad90-a2311634d312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/nmt\n",
      "Cloning into 'MT-Preparation'...\n",
      "remote: Enumerating objects: 227, done.\u001b[K\n",
      "remote: Counting objects: 100% (227/227), done.\u001b[K\n",
      "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
      "remote: Total 227 (delta 115), reused 186 (delta 94), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (227/227), 54.89 KiB | 213.00 KiB/s, done.\n",
      "Resolving deltas: 100% (115/115), done.\n"
     ]
    }
   ],
   "source": [
    "# Create a directory and clone the Github MT-Preparation repository\n",
    "!mkdir -p nmt\n",
    "%cd nmt\n",
    "!git clone https://github.com/ymoslem/MT-Preparation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab8d2de-0962-45df-b418-7c50cc03b0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/venv/lib/python3.9/site-packages (from -r MT-Preparation/requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: pandas in /home/venv/lib/python3.9/site-packages (from -r MT-Preparation/requirements.txt (line 2)) (2.0.1)\n",
      "Requirement already satisfied: sentencepiece in /home/venv/lib/python3.9/site-packages (from -r MT-Preparation/requirements.txt (line 3)) (0.1.99)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/venv/lib/python3.9/site-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/venv/lib/python3.9/site-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/venv/lib/python3.9/site-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->-r MT-Preparation/requirements.txt (line 2)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the requirements\n",
    "!pip3 install -r MT-Preparation/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efd4cc-8d33-4420-bc1e-9cb08155410e",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "Example datasets:\n",
    "\n",
    "    EN-AR: https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/ar-en.txt.zip\n",
    "    EN-ES: https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-es.txt.zip\n",
    "    EN-FR: https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-fr.txt.zip\n",
    "    EN-RU: https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-ru.txt.zip\n",
    "    EN-ZH: https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-zh.txt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f0292bf-e949-4911-93e9-55c51a1823c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-30 23:52:35--  https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-fr.txt.zip\n",
      "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
      "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10014972 (9.6M) [application/zip]\n",
      "Saving to: ‘en-fr.txt.zip’\n",
      "\n",
      "en-fr.txt.zip       100%[===================>]   9.55M  1.89MB/s    in 14s     \n",
      "\n",
      "2023-06-30 23:52:51 (705 KB/s) - ‘en-fr.txt.zip’ saved [10014972/10014972]\n",
      "\n",
      "Archive:  en-fr.txt.zip\n",
      "  inflating: UN.en-fr.en             \n",
      "  inflating: UN.en-fr.fr             \n",
      "  inflating: README                  \n"
     ]
    }
   ],
   "source": [
    "# Download and unzip a dataset\n",
    "!wget https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-fr.txt.zip\n",
    "!unzip en-fr.txt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84cf5ca5-dcfd-49fb-9b9c-8de7c8290406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HTTP_PROXY'] = 'http://proxy.vmware.com:3128'\n",
    "os.environ['HTTPS_PROXY'] = 'http://proxy.vmware.com:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e63c6544-815e-41c1-8de5-844072e65793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape (rows, columns): (74067, 2)\n",
      "--- Rows with Empty Cells Deleted\t--> Rows: 74067\n",
      "--- Duplicates Deleted\t\t\t--> Rows: 60662\n",
      "--- Source-Copied Rows Deleted\t\t--> Rows: 60476\n",
      "--- Too Long Source/Target Deleted\t--> Rows: 59719\n",
      "--- HTML Removed\t\t\t--> Rows: 59719\n",
      "--- Rows will remain in true-cased\t--> Rows: 59719\n",
      "--- Rows with Empty Cells Deleted\t--> Rows: 59719\n",
      "--- Rows Shuffled\t\t\t--> Rows: 59719\n",
      "--- Source Saved: UN.en-fr.fr-filtered.fr\n",
      "--- Target Saved: UN.en-fr.en-filtered.en\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset\n",
    "# Arguments: source file, target file, source language, target language\n",
    "!python3 MT-Preparation/filtering/filter.py UN.en-fr.fr UN.en-fr.en fr en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37120b69-ab98-41f1-a8bc-8655ffc3de1c",
   "metadata": {},
   "source": [
    "### Tokenization / Sub-wording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37efb603-ec90-4c84-bd91-4133643dd82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-train_bpe.py\t1-train_unigram.py  2-subword.py  3-desubword.py\n"
     ]
    }
   ],
   "source": [
    "!ls MT-Preparation/subwording/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ac2fee0-2ba0-481d-a405-e9de11c4428c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=UN.en-fr.fr-filtered.fr --model_prefix=source --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: UN.en-fr.fr-filtered.fr\n",
      "  input_format: \n",
      "  model_prefix: source\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 50000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: UN.en-fr.fr-filtered.fr\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 59719 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=19614832\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9546% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=82\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999546\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 59719 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=13627710\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 61805 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 59719\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 48938\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 48938 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=23065 obj=12.0957 num_tokens=205556 num_tokens/piece=8.91203\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=16778 obj=8.81693 num_tokens=205778 num_tokens/piece=12.2648\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: source.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: source.vocab\n",
      "Done, training a SentencepPiece model for the Source finished successfully!\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=UN.en-fr.en-filtered.en --model_prefix=target --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: UN.en-fr.en-filtered.en\n",
      "  input_format: \n",
      "  model_prefix: target\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 50000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: UN.en-fr.en-filtered.en\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 59719 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=17772658\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9623% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=70\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999623\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 59719 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=11800957\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 46525 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 59719\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 44916\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 44916 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=19499 obj=11.8498 num_tokens=207239 num_tokens/piece=10.6282\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=13744 obj=8.60033 num_tokens=207645 num_tokens/piece=15.108\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: target.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: target.vocab\n",
      "Done, training a SentencepPiece model for the Target finished successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train a SentencePiece model for subword tokenization\n",
    "!python3 MT-Preparation/subwording/1-train_unigram.py UN.en-fr.fr-filtered.fr UN.en-fr.en-filtered.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "468dfde6-c844-4b48-aee5-04f276b9ad2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT-Preparation\tUN.en-fr.en-filtered.en  en-fr.txt.zip\ttarget.model\n",
      "README\t\tUN.en-fr.fr\t\t source.model\ttarget.vocab\n",
      "UN.en-fr.en\tUN.en-fr.fr-filtered.fr  source.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6cec949-86ac-49bf-acf5-e79541f3827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Model: source.model\n",
      "Target Model: target.model\n",
      "Source Dataset: UN.en-fr.fr-filtered.fr\n",
      "Target Dataset: UN.en-fr.en-filtered.en\n",
      "Done subwording the source file! Output: UN.en-fr.fr-filtered.fr.subword\n",
      "Done subwording the target file! Output: UN.en-fr.en-filtered.en.subword\n"
     ]
    }
   ],
   "source": [
    "# Subword the dataset\n",
    "!python3 MT-Preparation/subwording/2-subword.py source.model target.model UN.en-fr.fr-filtered.fr UN.en-fr.en-filtered.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6be1ca0-617e-443e-9dd2-5dbd0af787e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notant qu'elle a adopté à sa dixième session extraordinaire des principes directeurs essentiels pour progresser sur la voie du désarmement général et completRésolution S-10/2.,\n",
      "l) Le rapport du Secrétaire général intitulé « Plan de campagne pour la mise en œuvre de la Déclaration du Millénaire »A/56/326 ; voir également le rapport du Secrétaire général sur l'application de la Déclaration du Millénaire adoptée par l'Organisation des Nations Unies (A/58/323), par. 23., en particulier ses paragraphes 56 à 61,\n",
      "Considérant que tous les États, notamment ceux qui sont particulièrement avancés dans le domaine spatial, doivent s'employer activement à empêcher une course aux armements dans l'espace, condition essentielle pour promouvoir et renforcer la coopération internationale touchant l'exploration et l'utilisation de l'espace à des fins pacifiques,\n",
      "-----\n",
      "Noting that essential guidelines for progress towards general and complete disarmament were adopted at the tenth special session of the General Assembly,Resolution S-10/2.\n",
      "(l) The report of the Secretary-General entitled \"Road map towards implementation of the United Nations Millennium Declaration\",A/56/326; see also the report of the Secretary-General on the implementation of the United Nations Millennium Declaration (A/58/323), para. 23. in particular paragraphs 56 to 61 thereof,\n",
      "Recognizing that all States, in particular those with major space capabilities, should contribute actively to the goal of preventing an arms race in outer space as an essential condition for the promotion and strengthening of international cooperation in the exploration and use of outer space for peaceful purposes,\n"
     ]
    }
   ],
   "source": [
    "# First 3 lines before subwording\n",
    "!head -n 3 UN.en-fr.fr-filtered.fr && echo \"-----\" && head -n 3 UN.en-fr.en-filtered.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "063cadef-4e42-40b7-9b1a-d4bc6f1b5efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁Notant ▁qu ' elle ▁a ▁adopté ▁à ▁sa ▁dixième ▁session ▁extraordinaire ▁des ▁principes ▁directeurs ▁essentiels ▁pour ▁progresser ▁sur ▁la ▁voie ▁du ▁désarmement ▁général ▁et ▁complet Résolution ▁S - 1 0 / 2 .,\n",
      "▁l ) ▁Le ▁rapport ▁du ▁Secrétaire ▁général ▁intitulé ▁« ▁Plan ▁de ▁campagne ▁pour ▁la ▁mise ▁en ▁œuvre ▁de ▁la ▁Déclaration ▁du ▁Millénaire ▁» A / 5 6 / 3 2 6 ▁; ▁voir ▁également ▁le ▁rapport ▁du ▁Secrétaire ▁général ▁sur ▁l ' application ▁de ▁la ▁Déclaration ▁du ▁Millénaire ▁adoptée ▁par ▁l ' Organisation ▁des ▁Nations ▁Unies ▁( A / 5 8 / 3 2 3 ), ▁par . ▁ 2 3 ., ▁en ▁particulier ▁ses ▁paragraphe s ▁ 5 6 ▁à ▁ 6 1 ,\n",
      "▁Considérant ▁que ▁tous ▁les ▁États , ▁notamment ▁ceux ▁qui ▁sont ▁ particulièrement ▁avancés ▁d ans ▁le ▁domaine ▁spatial , ▁doivent ▁s ' employer ▁activement ▁à ▁empêcher ▁une ▁cours e ▁aux ▁armements ▁d ans ▁l ' espace , ▁condition ▁essentielle ▁pour ▁promouvoir ▁et ▁renforcer ▁la ▁coopération ▁internationale ▁touchant ▁l ' exploration ▁et ▁l ' utilisation ▁de ▁l ' espace ▁à ▁des ▁fins ▁pacifiques ,\n",
      "---\n",
      "▁Noting ▁that ▁essential ▁guidelines ▁for ▁progress ▁towards ▁general ▁and ▁complete ▁disarmament ▁were ▁adopted ▁at ▁the ▁tenth ▁special ▁session ▁of ▁the ▁General ▁Assembly , Resolution ▁S - 1 0 / 2 .\n",
      "▁( l ) ▁The ▁report ▁of ▁the ▁Secretary - General ▁entitled ▁\" R oad ▁map ▁towards ▁implementation ▁of ▁the ▁Unit ed ▁Nations ▁Millennium ▁Declaration \", A / 5 6 / 3 2 6 ; ▁see ▁also ▁the ▁report ▁of ▁the ▁Secretary - General ▁on ▁the ▁implementation ▁of ▁the ▁Unit ed ▁Nations ▁Millennium ▁Declaration ▁( A / 5 8 / 3 2 3 ), ▁para . ▁ 2 3 . ▁in ▁particular ▁paragraphs ▁ 5 6 ▁to ▁ 6 1 ▁thereof ,\n",
      "▁Recognizing ▁that ▁all ▁States , ▁in ▁particular ▁th ose ▁with ▁major ▁space ▁capabilities , ▁should ▁contribute ▁actively ▁to ▁the ▁goal ▁of ▁preventing ▁an ▁arms ▁race ▁in ▁out er ▁space ▁as ▁an ▁essential ▁condition ▁for ▁the ▁promotion ▁and ▁strengthening ▁of ▁international ▁cooperation ▁in ▁the ▁exploration ▁and ▁use ▁of ▁out er ▁space ▁for ▁peaceful ▁purposes ,\n"
     ]
    }
   ],
   "source": [
    "# First 3 lines after subwording\n",
    "!head -n 3 UN.en-fr.fr-filtered.fr.subword && echo \"---\" && head -n 3 UN.en-fr.en-filtered.en.subword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ee20fa-dd69-4cbf-a191-72048e07dae0",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "\n",
    "We usually split our dataset into 3 portions:\n",
    "\n",
    "    1. training dataset - used for training the model;\n",
    "    2. development dataset - used to run regular validations during the training to help improve the model parameters; and\n",
    "    3. testing dataset - a holdout dataset used after the model finishes training to finally evaluate the model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "160280d6-54a8-4516-b0b2-06984a5691c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (59719, 2)\n",
      "--- Empty Cells Deleted --> Rows: 59719\n",
      "--- Wrote Files\n",
      "Done!\n",
      "Output files\n",
      "UN.en-fr.fr-filtered.fr.subword.train\n",
      "UN.en-fr.en-filtered.en.subword.train\n",
      "UN.en-fr.fr-filtered.fr.subword.dev\n",
      "UN.en-fr.en-filtered.en.subword.dev\n",
      "UN.en-fr.fr-filtered.fr.subword.test\n",
      "UN.en-fr.en-filtered.en.subword.test\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training set, development set, and test set\n",
    "# Development and test sets should be between 1000 and 5000 segments (here we chose 2000)\n",
    "!python3 MT-Preparation/train_dev_split/train_dev_test_split.py 2000 2000 UN.en-fr.fr-filtered.fr.subword UN.en-fr.en-filtered.en.subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5acd0dcd-df2d-4700-8fbf-784bcefa6ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2000 UN.en-fr.en-filtered.en.subword.dev\n",
      "    2000 UN.en-fr.en-filtered.en.subword.test\n",
      "   55719 UN.en-fr.en-filtered.en.subword.train\n",
      "    2000 UN.en-fr.fr-filtered.fr.subword.dev\n",
      "    2000 UN.en-fr.fr-filtered.fr.subword.test\n",
      "   55719 UN.en-fr.fr-filtered.fr.subword.train\n",
      "  119438 total\n"
     ]
    }
   ],
   "source": [
    "# Line count for the subworded train, dev, test datatest\n",
    "!wc -l *.subword.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc8bbdc6-4bcc-4866-959f-602de1deda7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is: FirstName SecondName \n",
      "\n",
      "---First line---\n",
      "==> UN.en-fr.en-filtered.en.subword.train <==\n",
      "▁( l ) ▁The ▁report ▁of ▁the ▁Secretary - General ▁entitled ▁\" R oad ▁map ▁towards ▁implementation ▁of ▁the ▁Unit ed ▁Nations ▁Millennium ▁Declaration \", A / 5 6 / 3 2 6 ; ▁see ▁also ▁the ▁report ▁of ▁the ▁Secretary - General ▁on ▁the ▁implementation ▁of ▁the ▁Unit ed ▁Nations ▁Millennium ▁Declaration ▁( A / 5 8 / 3 2 3 ), ▁para . ▁ 2 3 . ▁in ▁particular ▁paragraphs ▁ 5 6 ▁to ▁ 6 1 ▁thereof ,\n",
      "\n",
      "==> UN.en-fr.fr-filtered.fr.subword.train <==\n",
      "▁l ) ▁Le ▁rapport ▁du ▁Secrétaire ▁général ▁intitulé ▁« ▁Plan ▁de ▁campagne ▁pour ▁la ▁mise ▁en ▁œuvre ▁de ▁la ▁Déclaration ▁du ▁Millénaire ▁» A / 5 6 / 3 2 6 ▁; ▁voir ▁également ▁le ▁rapport ▁du ▁Secrétaire ▁général ▁sur ▁l ' application ▁de ▁la ▁Déclaration ▁du ▁Millénaire ▁adoptée ▁par ▁l ' Organisation ▁des ▁Nations ▁Unies ▁( A / 5 8 / 3 2 3 ), ▁par . ▁ 2 3 ., ▁en ▁particulier ▁ses ▁paragraphe s ▁ 5 6 ▁à ▁ 6 1 ,\n",
      "\n",
      "==> UN.en-fr.en-filtered.en.subword.dev <==\n",
      "▁ 5 . ▁Reaffirms ▁its ▁decision , ▁with ▁a ▁view ▁to ▁further ▁development ▁of ▁the ▁Register , ▁to ▁keep ▁the ▁scope ▁of ▁and ▁participation ▁in ▁the ▁Register ▁under ▁review ▁and , ▁to ▁that ▁end :\n",
      "\n",
      "==> UN.en-fr.fr-filtered.fr.subword.dev <==\n",
      "▁ 5 . ▁Réaffirme ▁sa ▁décision ▁de ▁continuer ▁à ▁examiner ▁la ▁portée ▁du ▁Registre ▁ainsi ▁que ▁la ▁participation ▁à ▁celui - ci , ▁en ▁vue ▁de ▁l ' améliorer ▁encore , ▁et ▁à ▁cet ▁effet :\n",
      "\n",
      "==> UN.en-fr.en-filtered.en.subword.test <==\n",
      "▁Recalling ▁also ▁Security ▁Council ▁resolution ▁ 1 4 1 0 ▁( 2 0 0 2 ) ▁of ▁ 1 7 ▁May ▁ 2 0 0 2 , ▁by ▁which ▁the ▁Council ▁established ▁the ▁Unit ed ▁Nations ▁Mission ▁of ▁Support ▁in ▁East ▁Timor ▁as ▁of ▁ 2 0 ▁May ▁ 2 0 0 2 ▁for ▁an ▁initial ▁period ▁of ▁twelve ▁months , ▁and ▁its ▁subsequent ▁resolution ▁ 1 4 8 0 ▁( 2 0 0 3 ) ▁of ▁ 1 9 ▁May ▁ 2 0 0 3 , ▁by ▁which ▁the ▁Council ▁extended ▁the ▁mandate ▁of ▁the ▁Mission ▁unti l ▁ 2 0 ▁May ▁ 2 0 0 4 ,\n",
      "\n",
      "==> UN.en-fr.fr-filtered.fr.subword.test <==\n",
      "▁Rappelant ▁également ▁la ▁résolution ▁ 1 4 1 0 ▁( 2 0 0 2 ) ▁du ▁ 1 7 ▁mai ▁ 2 0 0 2 , ▁par ▁laquelle ▁le ▁Conseil ▁de ▁sécurité ▁a ▁créé ▁la ▁Mission ▁d ' appui ▁des ▁Nations ▁Unies ▁au ▁Timor ▁oriental ▁pour ▁une ▁période ▁initiale ▁de ▁douz e ▁mois ▁commençant ▁le ▁ 2 0 ▁mai ▁ 2 0 0 2 , ▁et ▁la ▁résolution ▁ 1 4 8 0 ▁( 2 0 0 3 ) ▁du ▁ 1 9 ▁mai ▁ 2 0 0 3 , ▁par ▁laquelle ▁il ▁a ▁prorogé ▁le ▁mandat ▁de ▁la ▁Mission ▁jus qu ' au ▁ 2 0 ▁mai ▁ 2 0 0 4 ,\n",
      "\n",
      "---Last line---\n",
      "==> UN.en-fr.en-filtered.en.subword.train <==\n",
      "▁Tak ing ▁into ▁account ▁the ▁deliberations ▁at ▁the ▁ 2 0 0 0 ▁substantive ▁session ▁of ▁the ▁Disarmament ▁Commission ▁in ▁Work ing ▁Group ▁II ▁on ▁agenda ▁item ▁ 5 , ▁entitled ▁\" Practic al ▁confidence - building ▁measures ▁in ▁the ▁field ▁of ▁convention al ▁arms \", See ▁Official ▁Record s ▁of ▁the ▁General ▁Assembly , ▁Fift y - fifth ▁Session , ▁Supplement ▁No . ▁ 4 2 ▁( A / 5 5 / 4 2 ), ▁para . ▁ 2 9 . ▁and ▁encouraging ▁the ▁Disarmament ▁Commission ▁to ▁continue ▁its ▁efforts ▁aimed ▁at ▁the ▁identification ▁of ▁such ▁measures ,\n",
      "\n",
      "==> UN.en-fr.fr-filtered.fr.subword.train <==\n",
      "▁Pren ant ▁en ▁considération ▁les ▁débats ▁tenus ▁par ▁le ▁Groupe ▁de ▁travail ▁II , ▁lors ▁de ▁la ▁session ▁de ▁fond ▁de ▁ 2 0 0 0 ▁de ▁la ▁Commission ▁du ▁désarmement , ▁sur ▁le ▁point ▁ 5 ▁de ▁l ' ordre ▁du ▁jour ▁intitulé ▁« Mesure s ▁concrètes ▁de ▁confiance ▁d ans ▁le ▁domaine ▁des ▁armes ▁classiques Voir ▁Document s ▁officiels ▁de ▁l ' Assemblée ▁générale , ▁cinquante - cinquième ▁session , ▁S upplément ▁no ▁ 4 2 ▁( A / 5 5 / 4 2 ), ▁par . ▁ 2 9 .», ▁et ▁encourageant ▁la ▁Commission ▁à ▁continuer ▁de ▁s ' efforcer ▁de ▁définir ▁de ▁telle s ▁mesures ,\n",
      "\n",
      "==> UN.en-fr.en-filtered.en.subword.dev <==\n",
      "▁Abst aining : ▁Albania , ▁A ndor ra , ▁Australia , ▁Belgium , ▁Bolivia , ▁Bosnia ▁and ▁Herzegovina , ▁Bulgaria , ▁Canada , ▁China , ▁Croatia , ▁C ze ch ▁Republic , ▁Denmark , ▁Estonia , ▁Georgia , ▁Greece , ▁Hungary , ▁Israel , ▁Latvia , ▁Lithuania , ▁Luxembourg , ▁M arshall ▁Islands , ▁Micronesia ▁( Federat ed ▁States ▁of ), ▁Moldova , ▁Montenegro , ▁Netherlands , ▁Palau , ▁Poland , ▁Republic ▁of ▁Korea , ▁Romania , ▁Serbia , ▁Slovakia , ▁Slovenia , ▁the ▁former ▁Yugoslav ▁Republic ▁of ▁Macedonia , ▁Tonga , ▁Turkey , ▁Ukraine\n",
      "\n",
      "==> UN.en-fr.fr-filtered.fr.subword.dev <==\n",
      "▁Se ▁sont ▁abstenus ▁: ▁ Albanie , ▁And orre , ▁Australie , ▁Belg ique , ▁Bolivie , ▁Bosnie - Herzégovine , ▁Bulgarie , ▁Canada , ▁Chine , ▁Croatie , ▁Dan e mark , ▁Estonie , ▁ex - République ▁yougoslave ▁de ▁Macédoine , ▁Géorgie , ▁Grèce , ▁Hongrie , ▁ Î les ▁Marshall , ▁Isra ë l , ▁Lettonie , ▁Lituanie , ▁Luxembourg , ▁Micronésie ▁( États ▁fédér és ▁de ), ▁Moldova , ▁Monténégro , ▁Palaos , ▁Pays - Bas , ▁Pologne , ▁République ▁de ▁Corée , ▁République ▁tchèque , ▁Roumanie , ▁Serbie , ▁Slovaquie , ▁Slovénie , ▁Tonga , ▁Turquie , ▁Ukraine\n",
      "\n",
      "==> UN.en-fr.en-filtered.en.subword.test <==\n",
      "▁ 2 1 . ▁Stresses ▁the ▁need ▁for ▁the ▁continued ▁regular ▁exchange ▁of ▁views ▁between ▁the ▁Committee , ▁the ▁Special ▁Rapporteur ▁and ▁other ▁relevant ▁Unit ed ▁Nations ▁mechanisms ▁and ▁bodies , ▁as ▁well ▁as ▁for ▁the ▁pursu ance ▁of ▁cooperation ▁with ▁relevant ▁Unit ed ▁Nations ▁programmes , ▁notably ▁the ▁Unit ed ▁Nations ▁Crime ▁Prevention ▁and ▁Criminal ▁Justice ▁Programme , ▁with ▁a ▁view ▁to ▁enhancing ▁further ▁their ▁effectiveness ▁and ▁cooperation ▁on ▁issues ▁relating ▁to ▁torture , ▁inter ▁alia , ▁by ▁improving ▁their ▁coordination ;\n",
      "\n",
      "==> UN.en-fr.fr-filtered.fr.subword.test <==\n",
      "▁ 2 1 . ▁Souligne ▁que ▁les ▁échanges ▁de ▁vues ▁réguliers ▁entre ▁le ▁Comité , ▁le ▁Rapporteur ▁spécial ▁et ▁les ▁autres ▁instances ▁et ▁organes ▁compétents ▁des ▁Nations ▁Unies , ▁ainsi ▁que ▁la ▁coopération ▁avec ▁les ▁programmes ▁pertinents ▁des ▁Nations ▁Unies , ▁en ▁particulier ▁le ▁Programme ▁des ▁Nations ▁Unies ▁en ▁matière ▁de ▁prévention ▁du ▁crime ▁et ▁de ▁justice ▁pénale , ▁doivent ▁continuer , ▁l ' objectif ▁étant ▁de ▁ rendre ▁ces ▁échanges ▁de ▁vues ▁et ▁ cette ▁coopération ▁plus ▁efficaces ▁en ▁ce ▁qui ▁concerne ▁les ▁questions ▁relatives ▁à ▁la ▁torture , ▁gr â ce ▁notamment ▁à ▁une ▁meilleure ▁coordination ;\n"
     ]
    }
   ],
   "source": [
    "# Check the first and last line from each dataset\n",
    "\n",
    "# -------------------------------------------\n",
    "# Change this cell to print your name\n",
    "!echo -e \"My name is: FirstName SecondName \\n\"\n",
    "# -------------------------------------------\n",
    "\n",
    "!echo \"---First line---\"\n",
    "!head -n 1 *.{train,dev,test}\n",
    "\n",
    "!echo -e \"\\n---Last line---\"\n",
    "!tail -n 1 *.{train,dev,test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ea26c12-6c4c-455b-a51d-0ff609bae23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting OpenNMT-py\n",
      "  Downloading OpenNMT_py-3.3-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.9/242.9 kB\u001b[0m \u001b[31m518.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch<2.1,>=1.13 in /home/venv/lib/python3.9/site-packages (from OpenNMT-py) (2.0.0+cu117)\n",
      "Collecting configargparse (from OpenNMT-py)\n",
      "  Downloading ConfigArgParse-1.5.5-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: ctranslate2<4,>=3.2 in /home/venv/lib/python3.9/site-packages (from OpenNMT-py) (3.16.0)\n",
      "Requirement already satisfied: tensorboard>=2.3 in /home/venv/lib/python3.9/site-packages (from OpenNMT-py) (2.12.3)\n",
      "Requirement already satisfied: flask in /home/venv/lib/python3.9/site-packages (from OpenNMT-py) (2.3.2)\n",
      "Collecting waitress (from OpenNMT-py)\n",
      "  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyonmttok<2,>=1.35 (from OpenNMT-py)\n",
      "  Downloading pyonmttok-1.37.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/venv/lib/python3.9/site-packages (from OpenNMT-py) (6.0)\n",
      "Collecting sacrebleu (from OpenNMT-py)\n",
      "  Using cached sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "Collecting rapidfuzz (from OpenNMT-py)\n",
      "  Downloading rapidfuzz-3.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyahocorasick (from OpenNMT-py)\n",
      "  Downloading pyahocorasick-2.0.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fasttext-wheel (from OpenNMT-py)\n",
      "  Downloading fasttext_wheel-0.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/venv/lib/python3.9/site-packages (from ctranslate2<4,>=3.2->OpenNMT-py) (1.23.5)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/venv/lib/python3.9/site-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/venv/lib/python3.9/site-packages (from tensorboard>=2.3->OpenNMT-py) (1.54.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/venv/lib/python3.9/site-packages (from tensorboard>=2.3->OpenNMT-py) (2.19.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/venv/lib/python3.9/site-packages (from tensorboard>=2.3->OpenNMT-py) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/venv/lib/python3.9/site-packages (from tensorboard>=2.3->OpenNMT-py) (3.4.3)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/venv/lib/python3.9/site-packages (from tensorboard>=2.3->OpenNMT-py) (4.23.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/venv/lib/python3.9/site-packages (from tensorboard>=2.3->OpenNMT-py) (2.30.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/venv/lib/python3.9/site-packages (from tensorboard>=2.3->OpenNMT-py) (67.7.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/venv/lib/python3.9/site-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/venv/lib/python3.9/site-packages (from tensorboard>=2.3->OpenNMT-py) (2.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/venv/lib/python3.9/site-packages (from tensorboard>=2.3->OpenNMT-py) (0.40.0)\n",
      "Requirement already satisfied: filelock in /home/venv/lib/python3.9/site-packages (from torch<2.1,>=1.13->OpenNMT-py) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /home/venv/lib/python3.9/site-packages (from torch<2.1,>=1.13->OpenNMT-py) (4.5.0)\n",
      "Requirement already satisfied: sympy in /home/venv/lib/python3.9/site-packages (from torch<2.1,>=1.13->OpenNMT-py) (1.12)\n",
      "Requirement already satisfied: networkx in /home/venv/lib/python3.9/site-packages (from torch<2.1,>=1.13->OpenNMT-py) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/venv/lib/python3.9/site-packages (from torch<2.1,>=1.13->OpenNMT-py) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/venv/lib/python3.9/site-packages (from torch<2.1,>=1.13->OpenNMT-py) (2.0.0)\n",
      "Requirement already satisfied: cmake in /home/venv/lib/python3.9/site-packages (from triton==2.0.0->torch<2.1,>=1.13->OpenNMT-py) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/venv/lib/python3.9/site-packages (from triton==2.0.0->torch<2.1,>=1.13->OpenNMT-py) (16.0.3)\n",
      "Collecting pybind11>=2.2 (from fasttext-wheel->OpenNMT-py)\n",
      "  Downloading pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.3/222.3 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: itsdangerous>=2.1.2 in /home/venv/lib/python3.9/site-packages (from flask->OpenNMT-py) (2.1.2)\n",
      "Requirement already satisfied: click>=8.1.3 in /home/venv/lib/python3.9/site-packages (from flask->OpenNMT-py) (8.1.3)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/venv/lib/python3.9/site-packages (from flask->OpenNMT-py) (1.6.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /home/venv/lib/python3.9/site-packages (from flask->OpenNMT-py) (6.6.0)\n",
      "Collecting portalocker (from sacrebleu->OpenNMT-py)\n",
      "  Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: regex in /home/venv/lib/python3.9/site-packages (from sacrebleu->OpenNMT-py) (2023.6.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/venv/lib/python3.9/site-packages (from sacrebleu->OpenNMT-py) (0.9.0)\n",
      "Collecting colorama (from sacrebleu->OpenNMT-py)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting lxml (from sacrebleu->OpenNMT-py)\n",
      "  Using cached lxml-4.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/venv/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/venv/lib/python3.9/site-packages (from importlib-metadata>=3.6.0->flask->OpenNMT-py) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/venv/lib/python3.9/site-packages (from jinja2->torch<2.1,>=1.13->OpenNMT-py) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/venv/lib/python3.9/site-packages (from sympy->torch<2.1,>=1.13->OpenNMT-py) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/venv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/venv/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->OpenNMT-py) (3.2.2)\n",
      "Installing collected packages: waitress, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, lxml, configargparse, colorama, sacrebleu, fasttext-wheel, OpenNMT-py\n",
      "Successfully installed OpenNMT-py-3.3 colorama-0.4.6 configargparse-1.5.5 fasttext-wheel-0.9.2 lxml-4.9.2 portalocker-2.7.0 pyahocorasick-2.0.0 pybind11-2.10.4 pyonmttok-1.37.1 rapidfuzz-3.1.1 sacrebleu-2.3.1 waitress-2.1.2\n"
     ]
    }
   ],
   "source": [
    "# Install OpenNMT-py 3.x\n",
    "!pip3 install OpenNMT-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20f760bf-a5e2-4f79-b818-48b9a0963204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT-Preparation\t\t\t       UN.en-fr.fr-filtered.fr.subword\n",
      "README\t\t\t\t       UN.en-fr.fr-filtered.fr.subword.dev\n",
      "UN.en-fr.en\t\t\t       UN.en-fr.fr-filtered.fr.subword.test\n",
      "UN.en-fr.en-filtered.en\t\t       UN.en-fr.fr-filtered.fr.subword.train\n",
      "UN.en-fr.en-filtered.en.subword        en-fr.txt.zip\n",
      "UN.en-fr.en-filtered.en.subword.dev    source.model\n",
      "UN.en-fr.en-filtered.en.subword.test   source.vocab\n",
      "UN.en-fr.en-filtered.en.subword.train  target.model\n",
      "UN.en-fr.fr\t\t\t       target.vocab\n",
      "UN.en-fr.fr-filtered.fr\n"
     ]
    }
   ],
   "source": [
    "# Open the folder where you saved your prepapred datasets from the first exercise\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45bbb1bf-7619-4920-8fc4-2807732b369f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/nmt'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcfcba1-a9d8-44e6-9afe-4047bf07c9f5",
   "metadata": {},
   "source": [
    "### Create the Training Configuration File\n",
    "\n",
    "The following config file matches most of the recommended values for the Transformer model [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762). As the current dataset is small, we reduced the following values:\n",
    "\n",
    "    train_steps - for datasets with a few millions of sentences, consider using a value between 100000 and 200000, or more! Enabling the option early_stopping can help stop the training when there is no considerable improvement.\n",
    "    valid_steps - 10000 can be good if the value train_steps is big enough.\n",
    "    warmup_steps - obviously, its value must be less than train_steps. Try 4000 and 8000 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "740b39fe-14a6-42e3-bdae-e486810913f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the YAML configuration file\n",
    "# On a regular machine, you can create it manually or with nano\n",
    "# Note here we are using some smaller values because the dataset is small\n",
    "# For larger datasets, consider increasing: train_steps, valid_steps, warmup_steps, save_checkpoint_steps, keep_checkpoint\n",
    "\n",
    "config = '''# config.yaml\n",
    "\n",
    "\n",
    "## Where the samples will be written\n",
    "save_data: run\n",
    "\n",
    "# Training files\n",
    "data:\n",
    "    corpus_1:\n",
    "        path_src: UN.en-fr.fr-filtered.fr.subword.train\n",
    "        path_tgt: UN.en-fr.en-filtered.en.subword.train\n",
    "        transforms: [filtertoolong]\n",
    "    valid:\n",
    "        path_src: UN.en-fr.fr-filtered.fr.subword.dev\n",
    "        path_tgt: UN.en-fr.en-filtered.en.subword.dev\n",
    "        transforms: [filtertoolong]\n",
    "\n",
    "# Vocabulary files, generated by onmt_build_vocab\n",
    "src_vocab: run/source.vocab\n",
    "tgt_vocab: run/target.vocab\n",
    "\n",
    "# Vocabulary size - should be the same as in sentence piece\n",
    "src_vocab_size: 50000\n",
    "tgt_vocab_size: 50000\n",
    "\n",
    "# Filter out source/target longer than n if [filtertoolong] enabled\n",
    "src_seq_length: 150\n",
    "src_seq_length: 150\n",
    "\n",
    "# Tokenization options\n",
    "src_subword_model: source.model\n",
    "tgt_subword_model: target.model\n",
    "\n",
    "# Where to save the log file and the output models/checkpoints\n",
    "log_file: train.log\n",
    "save_model: models/model.fren\n",
    "\n",
    "# Stop training if it does not imporve after n validations\n",
    "early_stopping: 4\n",
    "\n",
    "# Default: 5000 - Save a model checkpoint for each n\n",
    "save_checkpoint_steps: 1000\n",
    "\n",
    "# To save space, limit checkpoints to last n\n",
    "# keep_checkpoint: 3\n",
    "\n",
    "seed: 3435\n",
    "\n",
    "# Default: 100000 - Train the model to max n steps \n",
    "# Increase to 200000 or more for large datasets\n",
    "# For fine-tuning, add up the required steps to the original steps\n",
    "train_steps: 3000\n",
    "\n",
    "# Default: 10000 - Run validation after n steps\n",
    "valid_steps: 1000\n",
    "\n",
    "# Default: 4000 - for large datasets, try up to 8000\n",
    "warmup_steps: 1000\n",
    "report_every: 100\n",
    "\n",
    "# Number of GPUs, and IDs of GPUs\n",
    "world_size: 1\n",
    "gpu_ranks: [0]\n",
    "\n",
    "# Batching\n",
    "bucket_size: 262144\n",
    "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
    "batch_type: \"tokens\"\n",
    "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
    "valid_batch_size: 2048\n",
    "max_generator_batches: 2\n",
    "accum_count: [4]\n",
    "accum_steps: [0]\n",
    "\n",
    "# Optimization\n",
    "model_dtype: \"fp16\"\n",
    "optim: \"adam\"\n",
    "learning_rate: 2\n",
    "# warmup_steps: 8000\n",
    "decay_method: \"noam\"\n",
    "adam_beta2: 0.998\n",
    "max_grad_norm: 0\n",
    "label_smoothing: 0.1\n",
    "param_init: 0\n",
    "param_init_glorot: true\n",
    "normalization: \"tokens\"\n",
    "\n",
    "# Model\n",
    "encoder_type: transformer\n",
    "decoder_type: transformer\n",
    "position_encoding: true\n",
    "enc_layers: 6\n",
    "dec_layers: 6\n",
    "heads: 8\n",
    "hidden_size: 512\n",
    "word_vec_size: 512\n",
    "transformer_ff: 2048\n",
    "dropout_steps: [0]\n",
    "dropout: [0.1]\n",
    "attention_dropout: [0.1]\n",
    "'''\n",
    "\n",
    "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
    "  config_yaml.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "980c6a6b-1840-4b8f-9e02-97cd0595a206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# config.yaml\n",
      "\n",
      "\n",
      "## Where the samples will be written\n",
      "save_data: run\n",
      "\n",
      "# Training files\n",
      "data:\n",
      "    corpus_1:\n",
      "        path_src: UN.en-fr.fr-filtered.fr.subword.train\n",
      "        path_tgt: UN.en-fr.en-filtered.en.subword.train\n",
      "        transforms: [filtertoolong]\n",
      "    valid:\n",
      "        path_src: UN.en-fr.fr-filtered.fr.subword.dev\n",
      "        path_tgt: UN.en-fr.en-filtered.en.subword.dev\n",
      "        transforms: [filtertoolong]\n",
      "\n",
      "# Vocabulary files, generated by onmt_build_vocab\n",
      "src_vocab: run/source.vocab\n",
      "tgt_vocab: run/target.vocab\n",
      "\n",
      "# Vocabulary size - should be the same as in sentence piece\n",
      "src_vocab_size: 50000\n",
      "tgt_vocab_size: 50000\n",
      "\n",
      "# Filter out source/target longer than n if [filtertoolong] enabled\n",
      "src_seq_length: 150\n",
      "src_seq_length: 150\n",
      "\n",
      "# Tokenization options\n",
      "src_subword_model: source.model\n",
      "tgt_subword_model: target.model\n",
      "\n",
      "# Where to save the log file and the output models/checkpoints\n",
      "log_file: train.log\n",
      "save_model: models/model.fren\n",
      "\n",
      "# Stop training if it does not imporve after n validations\n",
      "early_stopping: 4\n",
      "\n",
      "# Default: 5000 - Save a model checkpoint for each n\n",
      "save_checkpoint_steps: 1000\n",
      "\n",
      "# To save space, limit checkpoints to last n\n",
      "# keep_checkpoint: 3\n",
      "\n",
      "seed: 3435\n",
      "\n",
      "# Default: 100000 - Train the model to max n steps \n",
      "# Increase to 200000 or more for large datasets\n",
      "# For fine-tuning, add up the required steps to the original steps\n",
      "train_steps: 3000\n",
      "\n",
      "# Default: 10000 - Run validation after n steps\n",
      "valid_steps: 1000\n",
      "\n",
      "# Default: 4000 - for large datasets, try up to 8000\n",
      "warmup_steps: 1000\n",
      "report_every: 100\n",
      "\n",
      "# Number of GPUs, and IDs of GPUs\n",
      "world_size: 1\n",
      "gpu_ranks: [0]\n",
      "\n",
      "# Batching\n",
      "bucket_size: 262144\n",
      "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
      "batch_type: \"tokens\"\n",
      "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
      "valid_batch_size: 2048\n",
      "max_generator_batches: 2\n",
      "accum_count: [4]\n",
      "accum_steps: [0]\n",
      "\n",
      "# Optimization\n",
      "model_dtype: \"fp16\"\n",
      "optim: \"adam\"\n",
      "learning_rate: 2\n",
      "# warmup_steps: 8000\n",
      "decay_method: \"noam\"\n",
      "adam_beta2: 0.998\n",
      "max_grad_norm: 0\n",
      "label_smoothing: 0.1\n",
      "param_init: 0\n",
      "param_init_glorot: true\n",
      "normalization: \"tokens\"\n",
      "\n",
      "# Model\n",
      "encoder_type: transformer\n",
      "decoder_type: transformer\n",
      "position_encoding: true\n",
      "enc_layers: 6\n",
      "dec_layers: 6\n",
      "heads: 8\n",
      "hidden_size: 512\n",
      "word_vec_size: 512\n",
      "transformer_ff: 2048\n",
      "dropout_steps: [0]\n",
      "dropout: [0.1]\n",
      "attention_dropout: [0.1]\n"
     ]
    }
   ],
   "source": [
    "# [Optional] Check the content of the configuration file\n",
    "!cat config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba801ade-8f81-464d-8b6d-8724a10f11be",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2162423-d91f-41d3-aed4-bc7cc8e5973f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# Find the number of CPUs/cores on the machine\n",
    "!nproc --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fad62d9d-4769-46c2-8f9c-c48a761355b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
      "[2023-07-01 00:04:41,904 INFO] Counter vocab from -1 samples.\n",
      "[2023-07-01 00:04:41,904 INFO] n_sample=-1: Build vocab on full datasets.\n",
      "[2023-07-01 00:04:43,343 INFO] * Transform statistics for corpus_1(25.00%):\n",
      "\t\t\t* FilterTooLongStats(filtered=1104)\n",
      "\n",
      "[2023-07-01 00:04:43,351 INFO] * Transform statistics for corpus_1(25.00%):\n",
      "\t\t\t* FilterTooLongStats(filtered=1040)\n",
      "\n",
      "[2023-07-01 00:04:43,354 INFO] * Transform statistics for corpus_1(25.00%):\n",
      "\t\t\t* FilterTooLongStats(filtered=1035)\n",
      "\n",
      "[2023-07-01 00:04:43,384 INFO] * Transform statistics for corpus_1(25.00%):\n",
      "\t\t\t* FilterTooLongStats(filtered=1025)\n",
      "\n",
      "[2023-07-01 00:04:43,439 INFO] Counters src: 14695\n",
      "[2023-07-01 00:04:43,439 INFO] Counters tgt: 11879\n"
     ]
    }
   ],
   "source": [
    "# Build Vocabulary\n",
    "\n",
    "# -config: path to your config.yaml file\n",
    "# -n_sample: use -1 to build vocabulary on all the segment in the training dataset\n",
    "# -num_threads: change it to match the number of CPUs to run it faster\n",
    "\n",
    "!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5505ba4a-5e9c-44c7-b6b4-c7ee59fa7020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: GRID V100-16C (UUID: GPU-87e669c5-28e4-11b2-bca1-dacc96a729c6)\n"
     ]
    }
   ],
   "source": [
    "# Check if the GPU is active\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e6aa9b8-49f3-4a47-96a3-932a91e9df46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "GRID V100-16C\n",
      "Free GPU memory: 14907.546875 out of: 16384.0\n"
     ]
    }
   ],
   "source": [
    "# Check if the GPU is visable to PyTorch\n",
    "\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "gpu_memory = torch.cuda.mem_get_info(0)\n",
    "print(\"Free GPU memory:\", gpu_memory[0]/1024**2, \"out of:\", gpu_memory[1]/1024**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22090343-58ce-4603-9dba-8fdfcb140aae",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b8b6e74-587e-422d-81f7-a54bb0578ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be3e8638-7f8c-48e8-8c8e-1d48ec5caf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-01 00:07:22,793 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
      "[2023-07-01 00:07:22,793 INFO] Parsed 2 corpora from -data.\n",
      "[2023-07-01 00:07:22,793 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
      "[2023-07-01 00:07:22,873 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '▁de', ',', \"'\", '▁et', '▁', '▁la']\n",
      "[2023-07-01 00:07:22,874 INFO] The decoder start token is: <s>\n",
      "[2023-07-01 00:07:22,874 INFO] Building model...\n",
      "[2023-07-01 00:07:23,557 INFO] Switching model to float32 for amp/apex_amp\n",
      "[2023-07-01 00:07:23,557 INFO] Non quantized layer compute is fp16\n",
      "[2023-07-01 00:07:25,009 INFO] NMTModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(14704, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(11888, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "    (transformer_layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (generator): Linear(in_features=512, out_features=11888, bias=True)\n",
      ")\n",
      "[2023-07-01 00:07:25,012 INFO] encoder: 26416128\n",
      "[2023-07-01 00:07:25,012 INFO] decoder: 37370480\n",
      "[2023-07-01 00:07:25,012 INFO] * number of parameters: 63786608\n",
      "[2023-07-01 00:07:25,013 INFO] Trainable parameters = {'torch.float32': 63786608, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
      "[2023-07-01 00:07:25,013 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
      "[2023-07-01 00:07:25,013 INFO]  * src vocab size = 14704\n",
      "[2023-07-01 00:07:25,013 INFO]  * tgt vocab size = 11888\n",
      "[2023-07-01 00:07:25,015 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 1\n",
      "[2023-07-01 00:07:25,015 INFO] Starting training on GPU: [0]\n",
      "[2023-07-01 00:07:25,015 INFO] Start training loop and validate every 1000 steps...\n",
      "[2023-07-01 00:07:25,015 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=150, tgt_seq_length=192))\n",
      "[2023-07-01 00:07:30,103 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 2\n",
      "[2023-07-01 00:07:35,558 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 3\n",
      "[2023-07-01 00:07:41,945 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 4\n",
      "[2023-07-01 00:07:47,812 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 5\n",
      "[2023-07-01 00:08:45,922 INFO] Step 100/ 3000; acc: 9.2; ppl: 1591.9; xent: 7.4; lr: 0.00028; sents:   26872; bsz: 4004/3523/67; 19794/17416 tok/s;     81 sec;\n",
      "[2023-07-01 00:09:19,705 INFO] Step 200/ 3000; acc: 30.4; ppl: 155.2; xent: 5.0; lr: 0.00056; sents:   24019; bsz: 4020/3518/60; 47605/41657 tok/s;    115 sec;\n",
      "[2023-07-01 00:09:53,479 INFO] Step 300/ 3000; acc: 41.5; ppl:  62.1; xent: 4.1; lr: 0.00084; sents:   25496; bsz: 4010/3523/64; 47492/41721 tok/s;    148 sec;\n",
      "[2023-07-01 00:10:27,161 INFO] Step 400/ 3000; acc: 47.8; ppl:  38.7; xent: 3.7; lr: 0.00112; sents:   25912; bsz: 4016/3534/65; 47697/41969 tok/s;    182 sec;\n",
      "[2023-07-01 00:11:00,947 INFO] Step 500/ 3000; acc: 55.2; ppl:  25.6; xent: 3.2; lr: 0.00140; sents:   26852; bsz: 4005/3529/67; 47421/41777 tok/s;    216 sec;\n",
      "[2023-07-01 00:11:34,676 INFO] Step 600/ 3000; acc: 62.3; ppl:  18.3; xent: 2.9; lr: 0.00168; sents:   26546; bsz: 4000/3520/66; 47443/41752 tok/s;    250 sec;\n",
      "[2023-07-01 00:12:08,507 INFO] Step 700/ 3000; acc: 67.3; ppl:  14.5; xent: 2.7; lr: 0.00196; sents:   29528; bsz: 3998/3535/74; 47276/41797 tok/s;    283 sec;\n",
      "[2023-07-01 00:12:42,279 INFO] Step 800/ 3000; acc: 70.0; ppl:  12.6; xent: 2.5; lr: 0.00224; sents:   24253; bsz: 4015/3523/61; 47553/41723 tok/s;    317 sec;\n",
      "[2023-07-01 00:13:15,953 INFO] Step 900/ 3000; acc: 71.1; ppl:  11.9; xent: 2.5; lr: 0.00252; sents:   25601; bsz: 4017/3527/64; 47723/41900 tok/s;    351 sec;\n",
      "[2023-07-01 00:13:18,482 INFO] * Transform statistics for corpus_1(100.00%):\n",
      "\t\t\t* FilterTooLongStats(filtered=19788)\n",
      "\n",
      "[2023-07-01 00:13:18,482 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 6\n",
      "[2023-07-01 00:13:22,558 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 7\n",
      "[2023-07-01 00:13:30,239 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 8\n",
      "[2023-07-01 00:13:39,114 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 9\n",
      "[2023-07-01 00:13:43,376 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 10\n",
      "[2023-07-01 00:14:44,529 INFO] Step 1000/ 3000; acc: 70.9; ppl:  11.9; xent: 2.5; lr: 0.00279; sents:   25763; bsz: 4013/3526/64; 18121/15923 tok/s;    440 sec;\n",
      "[2023-07-01 00:14:47,561 INFO] valid stats calculation and sentences rebuilding\n",
      "                           took: 3.030120849609375 s.\n",
      "[2023-07-01 00:14:47,564 INFO] Train perplexity: 38.5235\n",
      "[2023-07-01 00:14:47,564 INFO] Train accuracy: 52.5672\n",
      "[2023-07-01 00:14:47,564 INFO] Sentences processed: 260842\n",
      "[2023-07-01 00:14:47,564 INFO] Average bsz: 4010/3526/65\n",
      "[2023-07-01 00:14:47,564 INFO] Validation perplexity: 14.3771\n",
      "[2023-07-01 00:14:47,564 INFO] Validation accuracy: 68.5625\n",
      "[2023-07-01 00:14:47,564 INFO] Model is improving ppl: inf --> 14.3771.\n",
      "[2023-07-01 00:14:47,564 INFO] Model is improving acc: -inf --> 68.5625.\n",
      "[2023-07-01 00:14:47,569 INFO] Saving checkpoint models/model.fren_step_1000.pt\n",
      "[2023-07-01 00:15:23,688 INFO] Step 1100/ 3000; acc: 72.4; ppl:  11.0; xent: 2.4; lr: 0.00266; sents:   24348; bsz: 4017/3518/61; 41035/35940 tok/s;    479 sec;\n",
      "[2023-07-01 00:15:58,670 INFO] Step 1200/ 3000; acc: 74.1; ppl:  10.3; xent: 2.3; lr: 0.00255; sents:   28326; bsz: 4002/3532/71; 45759/40391 tok/s;    514 sec;\n",
      "[2023-07-01 00:16:33,698 INFO] Step 1300/ 3000; acc: 77.4; ppl:   8.9; xent: 2.2; lr: 0.00245; sents:   26521; bsz: 4008/3526/66; 45770/40262 tok/s;    549 sec;\n",
      "[2023-07-01 00:17:08,593 INFO] Step 1400/ 3000; acc: 79.4; ppl:   8.2; xent: 2.1; lr: 0.00236; sents:   25085; bsz: 4016/3528/63; 46032/40446 tok/s;    584 sec;\n",
      "[2023-07-01 00:17:43,524 INFO] Step 1500/ 3000; acc: 81.5; ppl:   7.5; xent: 2.0; lr: 0.00228; sents:   24893; bsz: 4013/3522/62; 45961/40339 tok/s;    619 sec;\n",
      "[2023-07-01 00:18:18,411 INFO] Step 1600/ 3000; acc: 82.7; ppl:   7.1; xent: 2.0; lr: 0.00221; sents:   27165; bsz: 4006/3535/68; 45937/40528 tok/s;    653 sec;\n",
      "[2023-07-01 00:18:53,530 INFO] Step 1700/ 3000; acc: 84.5; ppl:   6.7; xent: 1.9; lr: 0.00214; sents:   26307; bsz: 4011/3526/66; 45681/40156 tok/s;    689 sec;\n",
      "[2023-07-01 00:19:28,516 INFO] Step 1800/ 3000; acc: 85.4; ppl:   6.4; xent: 1.9; lr: 0.00208; sents:   26347; bsz: 4001/3519/66; 45741/40231 tok/s;    724 sec;\n",
      "[2023-07-01 00:19:33,856 INFO] * Transform statistics for corpus_1(100.00%):\n",
      "\t\t\t* FilterTooLongStats(filtered=19737)\n",
      "\n",
      "[2023-07-01 00:19:33,856 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 11\n",
      "[2023-07-01 00:19:37,901 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 12\n",
      "[2023-07-01 00:19:41,919 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 13\n",
      "[2023-07-01 00:19:50,343 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 14\n",
      "[2023-07-01 00:19:54,155 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 15\n",
      "[2023-07-01 00:20:53,729 INFO] Step 1900/ 3000; acc: 87.6; ppl:   5.9; xent: 1.8; lr: 0.00203; sents:   27427; bsz: 4007/3536/69; 18810/16599 tok/s;    809 sec;\n",
      "[2023-07-01 00:21:28,765 INFO] Step 2000/ 3000; acc: 87.8; ppl:   5.8; xent: 1.8; lr: 0.00198; sents:   27371; bsz: 4005/3527/68; 45728/40266 tok/s;    844 sec;\n",
      "[2023-07-01 00:21:28,953 INFO] * Transform statistics for valid(100.00%):\n",
      "\t\t\t* FilterTooLongStats(filtered=135)\n",
      "\n",
      "[2023-07-01 00:21:31,964 INFO] valid stats calculation and sentences rebuilding\n",
      "                           took: 3.1955482959747314 s.\n",
      "[2023-07-01 00:21:31,966 INFO] Train perplexity: 17.1045\n",
      "[2023-07-01 00:21:31,966 INFO] Train accuracy: 66.921\n",
      "[2023-07-01 00:21:31,966 INFO] Sentences processed: 524632\n",
      "[2023-07-01 00:21:31,966 INFO] Average bsz: 4009/3526/66\n",
      "[2023-07-01 00:21:31,966 INFO] Validation perplexity: 7.45945\n",
      "[2023-07-01 00:21:31,966 INFO] Validation accuracy: 84.2573\n",
      "[2023-07-01 00:21:31,966 INFO] Model is improving ppl: 14.3771 --> 7.45945.\n",
      "[2023-07-01 00:21:31,966 INFO] Model is improving acc: 68.5625 --> 84.2573.\n",
      "[2023-07-01 00:21:31,972 INFO] Saving checkpoint models/model.fren_step_2000.pt\n",
      "[2023-07-01 00:22:08,009 INFO] Step 2100/ 3000; acc: 88.3; ppl:   5.7; xent: 1.7; lr: 0.00193; sents:   25123; bsz: 4017/3524/63; 40941/35920 tok/s;    883 sec;\n",
      "[2023-07-01 00:22:42,980 INFO] Step 2200/ 3000; acc: 89.0; ppl:   5.5; xent: 1.7; lr: 0.00188; sents:   25186; bsz: 4013/3520/63; 45902/40264 tok/s;    918 sec;\n",
      "[2023-07-01 00:23:17,872 INFO] Step 2300/ 3000; acc: 89.7; ppl:   5.4; xent: 1.7; lr: 0.00184; sents:   25655; bsz: 4012/3524/64; 45995/40401 tok/s;    953 sec;\n",
      "[2023-07-01 00:23:52,933 INFO] Step 2400/ 3000; acc: 90.1; ppl:   5.3; xent: 1.7; lr: 0.00180; sents:   25508; bsz: 4005/3518/64; 45695/40138 tok/s;    988 sec;\n",
      "[2023-07-01 00:24:28,112 INFO] Step 2500/ 3000; acc: 90.8; ppl:   5.2; xent: 1.6; lr: 0.00177; sents:   25753; bsz: 4014/3532/64; 45640/40162 tok/s;   1023 sec;\n",
      "[2023-07-01 00:25:03,164 INFO] Step 2600/ 3000; acc: 91.1; ppl:   5.1; xent: 1.6; lr: 0.00173; sents:   27481; bsz: 4006/3529/69; 45712/40277 tok/s;   1058 sec;\n",
      "[2023-07-01 00:25:38,265 INFO] Step 2700/ 3000; acc: 91.5; ppl:   5.0; xent: 1.6; lr: 0.00170; sents:   26089; bsz: 4008/3528/65; 45680/40206 tok/s;   1093 sec;\n",
      "[2023-07-01 00:25:46,317 INFO] * Transform statistics for corpus_1(100.00%):\n",
      "\t\t\t* FilterTooLongStats(filtered=19805)\n",
      "\n",
      "[2023-07-01 00:25:46,318 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 16\n",
      "[2023-07-01 00:25:54,576 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 17\n",
      "[2023-07-01 00:25:58,630 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 18\n",
      "[2023-07-01 00:26:07,930 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 19\n",
      "[2023-07-01 00:27:08,926 INFO] Step 2800/ 3000; acc: 92.6; ppl:   4.9; xent: 1.6; lr: 0.00167; sents:   26198; bsz: 4012/3526/65; 17702/15556 tok/s;   1184 sec;\n",
      "[2023-07-01 00:27:44,018 INFO] Step 2900/ 3000; acc: 92.7; ppl:   4.8; xent: 1.6; lr: 0.00164; sents:   25510; bsz: 4011/3525/64; 45725/40184 tok/s;   1219 sec;\n",
      "[2023-07-01 00:28:19,018 INFO] Step 3000/ 3000; acc: 93.1; ppl:   4.8; xent: 1.6; lr: 0.00161; sents:   25271; bsz: 4016/3526/63; 45904/40294 tok/s;   1254 sec;\n",
      "[2023-07-01 00:28:19,266 INFO] * Transform statistics for valid(100.00%):\n",
      "\t\t\t* FilterTooLongStats(filtered=135)\n",
      "\n",
      "[2023-07-01 00:28:22,207 INFO] valid stats calculation and sentences rebuilding\n",
      "                           took: 3.185659408569336 s.\n",
      "[2023-07-01 00:28:22,209 INFO] Train perplexity: 11.4809\n",
      "[2023-07-01 00:28:22,209 INFO] Train accuracy: 74.9135\n",
      "[2023-07-01 00:28:22,209 INFO] Sentences processed: 782406\n",
      "[2023-07-01 00:28:22,209 INFO] Average bsz: 4010/3526/65\n",
      "[2023-07-01 00:28:22,209 INFO] Validation perplexity: 6.75576\n",
      "[2023-07-01 00:28:22,209 INFO] Validation accuracy: 87.4751\n",
      "[2023-07-01 00:28:22,209 INFO] Model is improving ppl: 7.45945 --> 6.75576.\n",
      "[2023-07-01 00:28:22,209 INFO] Model is improving acc: 84.2573 --> 87.4751.\n",
      "[2023-07-01 00:28:22,215 INFO] Saving checkpoint models/model.fren_step_3000.pt\n"
     ]
    }
   ],
   "source": [
    "# Train the NMT model\n",
    "!onmt_train -config config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a78b0-77fc-4303-9104-a5ff1bc8c2f4",
   "metadata": {},
   "source": [
    "### Translation\n",
    "\n",
    "Translation Options:\n",
    "\n",
    "    -model - specify the last model checkpoint name; try testing the quality of multiple checkpoints\n",
    "    -src - the subworded test dataset, source file\n",
    "    -output - give any file name to the new translation output file\n",
    "    -gpu - GPU ID, usually 0 if you have one GPU. Otherwise, it will translate on CPU, which would be slower.\n",
    "    -min_length - [optional] to avoid empty translations\n",
    "    -verbose - [optional] if you want to print translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1287e464-daf3-4e51-8563-2eb7460272f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-01 00:33:32,612 INFO] Loading checkpoint from models/model.fren_step_3000.pt\n",
      "[2023-07-01 00:33:33,294 INFO] Loading data into the model\n",
      "[2023-07-01 00:36:04,753 INFO] PRED SCORE: -0.2071, PRED PPL: 1.23 NB SENTENCES: 2000\n"
     ]
    }
   ],
   "source": [
    "# Translate the \"subworded\" source file of the test dataset\n",
    "# Change the model name, if needed.\n",
    "!onmt_translate -model models/model.fren_step_3000.pt -src UN.en-fr.fr-filtered.fr.subword.test -output UN.en.translated -gpu 0 -min_length 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fba64783-e20f-4b8c-9e5a-d6615499f450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁Recalling ▁also ▁Security ▁Council ▁resolution ▁ 1 4 0 ▁( 2 0 0 2 ) ▁of ▁ 1 7 ▁May ▁ 2 0 0 2 , ▁by ▁which ▁the ▁Council ▁established ▁the ▁Unit ed ▁Nations ▁Mission ▁of ▁Support ▁in ▁East ▁Timor ▁for ▁an ▁initial ▁period ▁of ▁twelve ▁months ▁as ▁from ▁ 2 0 ▁May ▁ 2 0 0 2 , ▁and ▁the ▁subsequent ▁resolution ▁ 1 8 0 3 ▁( 2 0 0 3 ) ▁of ▁ 1 9 ▁May ▁ 2 0 0 3 , ▁by ▁which ▁the ▁Council ▁extended ▁the ▁mandate ▁of ▁the ▁Mission ▁unti l ▁ 2 0 ▁May ▁ 2 0 0 4 ,\n",
      "▁Recalling ▁further ▁the ▁ 2 0 0 5 ▁World ▁Summit ▁Outcome , See ▁resolution ▁ 6 0 / 1 . ▁and ▁all ▁relevant ▁General ▁Assembly ▁resolutions , ▁in ▁particular ▁th ose ▁that ▁have ▁taken ▁place ▁in ▁the ▁economic , ▁social ▁and ▁related ▁fields , ▁including ▁its ▁resolution ▁ 6 0 / 2 6 5 ▁of ▁ 3 0 ▁June ▁ 2 0 0 6 , ▁entitled ▁\" Implementation ▁of ▁the ▁outcome ▁of ▁the ▁ 2 0 0 5 ▁World ▁Summit , ▁including ▁the ▁Millennium ▁Development ▁Goals ▁and ▁the ▁other ▁internationally ▁agreed ▁development ▁goals ,\n",
      "▁ 1 6 . ▁Welcomes ▁the ▁efforts ▁undertaken ▁so ▁far ▁to ▁enhance ▁the ▁security ▁conscious ness ▁within ▁the ▁Unit ed ▁Nations ▁system , ▁and ▁requests ▁the ▁Secretary - General ▁to ▁continue ▁to ▁take ▁the ▁necessary ▁measures ▁in ▁this ▁regard , ▁including ▁by ▁further ▁developing ▁and ▁implementing ▁a ▁unified ▁security ▁management ▁system , ▁as ▁well ▁as ▁by ▁disseminating ▁and ▁ensuring ▁the ▁implementation ▁of ▁the ▁security ▁procedures ▁and ▁regulations ▁and ▁by ▁ensuring ▁accountability ▁at ▁all ▁levels , ▁and ▁in ▁this ▁regard ▁recognizes ▁the ▁importance ▁of ▁the ▁work ▁of ▁the ▁Department ▁of ▁Safety ▁and ▁Security ▁of ▁the ▁Secretariat ;\n",
      "▁\"( d ) ▁An ▁equal ▁retirement ▁pension ▁shall ▁be ▁pay able ▁to ▁the ▁former ▁judge ▁who ▁has ▁been ▁paid ▁to ▁him ▁or ▁her ▁in ▁accord ance ▁with ▁article ▁ 3 4 ▁( a ) ▁of ▁ 1 9 ▁April ▁ 1 9 9 9 ▁and ▁who ▁has ▁ceased ▁to ▁hold ▁office ▁before ▁the ▁judge ▁who ▁has ▁ceased ▁to ▁hold ▁office ▁before ▁the ▁judge ▁who ▁has ▁ceased ▁to ▁hold ▁office ▁prior ▁to ▁the ▁pension , ▁the ▁amount ▁of ▁the ▁former ▁judge ▁who ▁has ▁been ▁transferred ▁to ▁the ▁present ▁article ▁and , ▁subject ▁to ▁a for ement ioned ▁paragraphs ▁( a ) ▁to ▁( a ) ▁and ▁( b ) ▁above , ▁subject ▁to ▁a ▁ 1 9 . 9 ;\n",
      "▁ 1 0 . ▁A lso ▁requests ▁the ▁Secretary - General ▁to ▁continue ▁his ▁efforts , ▁within ▁the ▁approved ▁budget , ▁to ▁make ▁all ▁available ▁the ▁Repertory ▁of ▁Practice ▁of ▁Unit ed ▁Nations ▁Organs ▁as ▁soon ▁as ▁possible ;\n"
     ]
    }
   ],
   "source": [
    "# Check the first 5 lines of the translation file\n",
    "!head -n 5 UN.en.translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f85ba01-2f25-4d3b-b749-45fe4b2477c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done desubwording! Output: UN.en.translated.desubword\n"
     ]
    }
   ],
   "source": [
    "# If needed install/update sentencepiece\n",
    "!pip3 install --upgrade -q sentencepiece\n",
    "\n",
    "# Desubword the translation file\n",
    "!python3 MT-Preparation/subwording/3-desubword.py target.model UN.en.translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc2d521d-5d8a-433e-be6c-8b3672c22b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recalling also Security Council resolution 140 (2002) of 17 May 2002, by which the Council established the United Nations Mission of Support in East Timor for an initial period of twelve months as from 20 May 2002, and the subsequent resolution 1803 (2003) of 19 May 2003, by which the Council extended the mandate of the Mission until 20 May 2004,\n",
      "Recalling further the 2005 World Summit Outcome,See resolution 60/1. and all relevant General Assembly resolutions, in particular those that have taken place in the economic, social and related fields, including its resolution 60/265 of 30 June 2006, entitled \"Implementation of the outcome of the 2005 World Summit, including the Millennium Development Goals and the other internationally agreed development goals,\n",
      "16. Welcomes the efforts undertaken so far to enhance the security consciousness within the United Nations system, and requests the Secretary-General to continue to take the necessary measures in this regard, including by further developing and implementing a unified security management system, as well as by disseminating and ensuring the implementation of the security procedures and regulations and by ensuring accountability at all levels, and in this regard recognizes the importance of the work of the Department of Safety and Security of the Secretariat;\n",
      "\"(d) An equal retirement pension shall be payable to the former judge who has been paid to him or her in accordance with article 34 (a) of 19 April 1999 and who has ceased to hold office before the judge who has ceased to hold office before the judge who has ceased to hold office prior to the pension, the amount of the former judge who has been transferred to the present article and, subject to aforementioned paragraphs (a) to (a) and (b) above, subject to a 19.9;\n",
      "10. Also requests the Secretary-General to continue his efforts, within the approved budget, to make all available the Repertory of Practice of United Nations Organs as soon as possible;\n"
     ]
    }
   ],
   "source": [
    "# Check the first 5 lines of the desubworded translation file\n",
    "!head -n 5 UN.en.translated.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "704cd1ac-daf8-489f-a490-b8c5c8867a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done desubwording! Output: UN.en-fr.en-filtered.en.subword.test.desubword\n"
     ]
    }
   ],
   "source": [
    "# Desubword the target file (reference) of the test dataset\n",
    "# Note: You might as well have split files *before* subwording during dataset preperation, \n",
    "# but sometimes datasets have tokeniztion issues, so this way you are sure the file is really untokenized.\n",
    "!python3 MT-Preparation/subwording/3-desubword.py target.model UN.en-fr.en-filtered.en.subword.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58b76947-2627-4acb-bbb1-8f867bd6c4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recalling also Security Council resolution 1410 (2002) of 17 May 2002, by which the Council established the United Nations Mission of Support in East Timor as of 20 May 2002 for an initial period of twelve months, and its subsequent resolution 1480 (2003) of 19 May 2003, by which the Council extended the mandate of the Mission until 20 May 2004,\n",
      "Recalling further the 2005 World Summit OutcomeSee resolution 60/1. and all relevant General Assembly resolutions, in particular those that have built upon the 2005 World Summit Outcome, in the economic, social and related fields, including General Assembly resolution 60/265 of 30 June 2006 on follow-up to the development outcome of the 2005 World Summit, including the Millennium Development Goals and the other internationally agreed development goals,\n",
      "16. Welcomes ongoing efforts to promote and enhance the security consciousness within the organizational culture of the United Nations system, and requests the Secretary-General to continue to take the necessary measures in this regard, including by further developing and implementing a unified security management system, as well as by disseminating and ensuring the implementation of the security procedures and regulations and by ensuring accountability at all levels, and in this regard recognizes the important work of the Department of Safety and Security of the Secretariat;\n",
      "\"(e) The divorced spouse of a former participant who separated before 1 April 1999 and, in the opinion of the Chief Executive Officer of the Fund, met all the other eligibility conditions in (a) and (b) above shall be entitled as from 1 April 1999 to a benefit equal to twice the minimum surviving spouse's benefit under article 34 (c), subject to the proviso that the amount of such benefit cannot exceed the amount payable to a surviving spouse of the former participant.\"\n",
      "10. Also requests the Secretary-General to continue his efforts, within the level of the currently approved budget, towards making available electronically all versions of the Repertory of Practice of United Nations Organs as early as possible;\n"
     ]
    }
   ],
   "source": [
    "# Check the first 5 lines of the desubworded reference\n",
    "!head -n 5 UN.en-fr.en-filtered.en.subword.test.desubword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ca89a-c33d-4dd2-a4ee-f42e6c68f107",
   "metadata": {},
   "source": [
    "### MT Evaluation\n",
    "\n",
    "There are several MT Evaluation metrics such as BLEU, TER, METEOR, COMET, BERTScore, among others.\n",
    "\n",
    "Here we are using BLEU. Files must be detokenized/desubworded beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37091436-517a-40bc-ac96-a9b969192958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-01 00:38:27--  https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 957 [text/plain]\n",
      "Saving to: ‘compute-bleu.py’\n",
      "\n",
      "compute-bleu.py     100%[===================>]     957  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-01 00:38:29 (19.2 MB/s) - ‘compute-bleu.py’ saved [957/957]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the BLEU script\n",
    "!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24eb2a8e-add1-4595-999b-0b6e9b9928f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /home/venv/lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: portalocker in /home/venv/lib/python3.9/site-packages (from sacrebleu) (2.7.0)\n",
      "Requirement already satisfied: regex in /home/venv/lib/python3.9/site-packages (from sacrebleu) (2023.6.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/venv/lib/python3.9/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/venv/lib/python3.9/site-packages (from sacrebleu) (1.23.5)\n",
      "Requirement already satisfied: colorama in /home/venv/lib/python3.9/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /home/venv/lib/python3.9/site-packages (from sacrebleu) (4.9.2)\n"
     ]
    }
   ],
   "source": [
    "# Install sacrebleu\n",
    "!pip3 install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4409798d-4994-42e8-b8ad-fe56a3a94e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 1st sentence: Recalling also Security Council resolution 1410 (2002) of 17 May 2002, by which the Council established the United Nations Mission of Support in East Timor as of 20 May 2002 for an initial period of twelve months, and its subsequent resolution 1480 (2003) of 19 May 2003, by which the Council extended the mandate of the Mission until 20 May 2004,\n",
      "MTed 1st sentence: Recalling also Security Council resolution 140 (2002) of 17 May 2002, by which the Council established the United Nations Mission of Support in East Timor for an initial period of twelve months as from 20 May 2002, and the subsequent resolution 1803 (2003) of 19 May 2003, by which the Council extended the mandate of the Mission until 20 May 2004,\n",
      "BLEU:  66.52958587194361\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the translation (without subwording)\n",
    "!python3 compute-bleu.py UN.en-fr.en-filtered.en.subword.test.desubword UN.en.translated.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a205218d-63a4-48d7-a0a7-2d88e0129794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
