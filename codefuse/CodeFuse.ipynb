{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27007fdc-dd0d-4414-a04a-cf463d2d74d0",
   "metadata": {},
   "source": [
    "# CodeFuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ec6714-ab42-4336-9778-44be01eab2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HTTP_PROXY'] = 'http://proxy.vmware.com:3128'\n",
    "os.environ['HTTPS_PROXY'] = 'http://proxy.vmware.com:3128'\n",
    "# os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbfd9653-addf-414d-8598-5c6e573e9a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.33.1\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: sentence-transformers, vllm\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d916acab-eab6-4465-a691-bb4beb683966",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.40.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (67.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97c4c50d-0dc7-4750-87d4-7f653992da82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55955210f0e34547b839a0090a5bcdee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/146 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c8a57a4bc04f6b8ff163680e885ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/131 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977d755503c242349760ce81986040fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32919e9ddf6a449da26474cb021d7521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/46.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484c0b2f9cab41108fe1a9ab5c759a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a32f0e06a54e6babb512de871e890f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00006.bin:   0%|          | 0.00/9.65G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a29034aade4425f811adeb3c576633a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00005-of-00006.bin:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aded9179cc4449bea937ac14b74f00d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab61a33ce47e433fa47462545324ca9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# language: Python\n",
      "def quick_sort(array):\n",
      "    if len(array) <= 1:\n",
      "        return array\n",
      "    else:\n",
      "        pivot = array[0]\n",
      "        less_than_pivot = [i for i in array[1:] if i <= pivot]\n",
      "        greater_than_pivot = [i for i in array[1:] if i > pivot]\n",
      "        return quick_sort(less_than_pivot) + [pivot] + quick_sort(greater_than_pivot)\n",
      "\n",
      "# Test the function\n",
      "print(quick_sort([3,6,8,10,1,2,1]))<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained((\"codefuse-ai/CodeFuse-13B\"))\n",
    "model = AutoModelForCausalLM.from_pretrained((\"codefuse-ai/CodeFuse-13B\"), device_map=\"auto\").half().eval()\n",
    "\n",
    "input_ids = tokenizer.encode(\"# language: Python\\ndef quick_sort(array):\\n\", return_tensors=\"pt\").to(\"cuda\")\n",
    "output_ids = model.generate(input_ids, max_new_tokens=200)\n",
    "\n",
    "print(tokenizer.decode(output_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a69120-57d7-4612-a2e9-981e22000ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protobuf                          4.24.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fb9b147-b1c1-49d8-a6d0-9374a33140c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==3.20.3\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m283.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.24.3\n",
      "    Uninstalling protobuf-4.24.3:\n",
      "      Successfully uninstalled protobuf-4.24.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorboard 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f00d5e6-e13a-457c-9a4b-515ac4f9aa49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /codefuse-ai/CodeFuse-CodeLlama-34B/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f59e0638d00>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 667f2f88-8fc0-4445-8084-fcff310173f6)')' thrown while requesting HEAD https://huggingface.co/codefuse-ai/CodeFuse-CodeLlama-34B/resolve/main/tokenizer_config.json\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /codefuse-ai/CodeFuse-CodeLlama-34B/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f59e06396f0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 34011954-bfce-472b-b80f-dc7b2c2851c5)')' thrown while requesting HEAD https://huggingface.co/codefuse-ai/CodeFuse-CodeLlama-34B/resolve/main/config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2a8e8cf3944349a50b9e3de1c605d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /codefuse-ai/CodeFuse-CodeLlama-34B/resolve/main/generation_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f59e8180ca0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 9699aabb-c450-403d-b84c-5dc33d97cf37)')' thrown while requesting HEAD https://huggingface.co/codefuse-ai/CodeFuse-CodeLlama-34B/resolve/main/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here is a Python function for quick sort:\\n\\n\\n```python\\ndef quicksort(arr):\\n    if len(arr) <= 1:\\n        return arr\\n    else:\\n        pivot = arr[len(arr) // 2]\\n        less = [x for x in arr if x < pivot]\\n        equal = [x for x in arr if x == pivot]\\n        greater = [x for x in arr if x > pivot]\\n        return quicksort(less) + equal + quicksort(greater)\\n```\\n\\n\\nThis function works by choosing a pivot element from the array (in this case, the middle element), and partitioning the other elements into two sub-arrays: one where all elements are less than the pivot, and one where all elements are greater than or equal to the pivot. It then recursively sorts the sub-arrays. This process continues until the base case of an array of length 0 or 1 is reached, at which point the function returns the array.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "\n",
    "# /home/jovyan/.cache/huggingface/hub/models--codefuse-ai--CodeFuse-CodeLlama-34B\n",
    "# codefuse-ai/CodeFuse-CodeLlama-34B\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codefuse-ai/CodeFuse-CodeLlama-34B\", trust_remote_code=True, use_fast=False, legacy=False)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"<unk>\")\n",
    "tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(\"</s>\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"codefuse-ai/CodeFuse-CodeLlama-34B\", trust_remote_code=True, device_map='auto')\n",
    "\n",
    "HUMAN_ROLE_START_TAG = \"<|role_start|>human<|role_end|>\"\n",
    "BOT_ROLE_START_TAG = \"<|role_start|>bot<|role_end|>\"\n",
    "\n",
    "text = f\"{HUMAN_ROLE_START_TAG}write a python function of quick sort.{BOT_ROLE_START_TAG}\" \n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, add_special_tokens=False).to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "        inputs=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=512,\n",
    "        top_p=0.95,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "gen_text = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58429bec-f51e-41ca-84ae-420b7e928112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
