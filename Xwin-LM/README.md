# Xwin-LM

Xwin-LM aims to develop and open-source alignment technologies for large language models, including supervised fine-tuning (SFT), reward models (RM), reject sampling, reinforcement learning from human feedback (RLHF), etc. Our first release, built-upon on the Llama2 base models, ranked TOP-1 on [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/). Notably, it's the first to surpass GPT-4 on this benchmark. The project will be continuously updated.

Note: from https://huggingface.co/Xwin-LM/Xwin-LM-7B-V0.1
