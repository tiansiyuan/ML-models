{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e168564-8013-429f-9870-a5a67e7f946b",
   "metadata": {},
   "source": [
    "# MusicLM: Generating Music From Text\n",
    "\n",
    "https://github.com/lucidrains/musiclm-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca0e654a-ab1d-4500-9858-e4cafa114d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting musiclm-pytorch\n",
      "  Downloading musiclm_pytorch-0.2.2-py3-none-any.whl (12 kB)\n",
      "Collecting accelerate (from musiclm-pytorch)\n",
      "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting audiolm-pytorch>=0.17.0 (from musiclm-pytorch)\n",
      "  Downloading audiolm_pytorch-1.2.15-py3-none-any.whl (39 kB)\n",
      "Collecting beartype (from musiclm-pytorch)\n",
      "  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting einops>=0.6 (from musiclm-pytorch)\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lion-pytorch (from musiclm-pytorch)\n",
      "  Downloading lion_pytorch-0.1.2-py3-none-any.whl (4.4 kB)\n",
      "Collecting vector-quantize-pytorch>=1.0.0 (from musiclm-pytorch)\n",
      "  Downloading vector_quantize_pytorch-1.6.30-py3-none-any.whl (13 kB)\n",
      "Collecting x-clip (from musiclm-pytorch)\n",
      "  Downloading x_clip-0.12.2-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.12 in /home/venv/lib/python3.9/site-packages (from musiclm-pytorch) (2.0.0+cu117)\n",
      "Requirement already satisfied: torchaudio in /home/venv/lib/python3.9/site-packages (from musiclm-pytorch) (2.0.1+cu117)\n",
      "Collecting ema-pytorch>=0.2.2 (from audiolm-pytorch>=0.17.0->musiclm-pytorch)\n",
      "  Downloading ema_pytorch-0.2.3-py3-none-any.whl (4.4 kB)\n",
      "Collecting encodec (from audiolm-pytorch>=0.17.0->musiclm-pytorch)\n",
      "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fairseq (from audiolm-pytorch>=0.17.0->musiclm-pytorch)\n",
      "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/venv/lib/python3.9/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.3.1)\n",
      "Collecting local-attention>=1.8.4 (from audiolm-pytorch>=0.17.0->musiclm-pytorch)\n",
      "  Downloading local_attention-1.8.6-py3-none-any.whl (8.1 kB)\n",
      "Requirement already satisfied: scikit-learn in /home/venv/lib/python3.9/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.3.0)\n",
      "Collecting sentencepiece (from audiolm-pytorch>=0.17.0->musiclm-pytorch)\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers in /home/venv/lib/python3.9/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.30.1)\n",
      "Requirement already satisfied: tqdm in /home/venv/lib/python3.9/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.65.0)\n",
      "Requirement already satisfied: filelock in /home/venv/lib/python3.9/site-packages (from torch>=1.12->musiclm-pytorch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /home/venv/lib/python3.9/site-packages (from torch>=1.12->musiclm-pytorch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /home/venv/lib/python3.9/site-packages (from torch>=1.12->musiclm-pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/venv/lib/python3.9/site-packages (from torch>=1.12->musiclm-pytorch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/venv/lib/python3.9/site-packages (from torch>=1.12->musiclm-pytorch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/venv/lib/python3.9/site-packages (from torch>=1.12->musiclm-pytorch) (2.0.0)\n",
      "Requirement already satisfied: cmake in /home/venv/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.12->musiclm-pytorch) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/venv/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.12->musiclm-pytorch) (16.0.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/venv/lib/python3.9/site-packages (from accelerate->musiclm-pytorch) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/venv/lib/python3.9/site-packages (from accelerate->musiclm-pytorch) (23.1)\n",
      "Requirement already satisfied: psutil in /home/venv/lib/python3.9/site-packages (from accelerate->musiclm-pytorch) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/venv/lib/python3.9/site-packages (from accelerate->musiclm-pytorch) (6.0)\n",
      "Collecting ftfy (from x-clip->musiclm-pytorch)\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex in /home/venv/lib/python3.9/site-packages (from x-clip->musiclm-pytorch) (2023.6.3)\n",
      "Requirement already satisfied: torchvision in /home/venv/lib/python3.9/site-packages (from x-clip->musiclm-pytorch) (0.15.1+cu117)\n",
      "Requirement already satisfied: cffi in /home/venv/lib/python3.9/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.15.1)\n",
      "Requirement already satisfied: cython in /home/venv/lib/python3.9/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.29.34)\n",
      "Collecting hydra-core<1.1,>=1.0.7 (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch)\n",
      "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting omegaconf<2.1 (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch)\n",
      "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /home/venv/lib/python3.9/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.3.1)\n",
      "Requirement already satisfied: bitarray in /home/venv/lib/python3.9/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.7.6)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/venv/lib/python3.9/site-packages (from ftfy->x-clip->musiclm-pytorch) (0.2.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/venv/lib/python3.9/site-packages (from jinja2->torch>=1.12->musiclm-pytorch) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/venv/lib/python3.9/site-packages (from scikit-learn->audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/venv/lib/python3.9/site-packages (from scikit-learn->audiolm-pytorch>=0.17.0->musiclm-pytorch) (3.1.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/venv/lib/python3.9/site-packages (from sympy->torch>=1.12->musiclm-pytorch) (1.3.0)\n",
      "Requirement already satisfied: requests in /home/venv/lib/python3.9/site-packages (from torchvision->x-clip->musiclm-pytorch) (2.30.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/venv/lib/python3.9/site-packages (from torchvision->x-clip->musiclm-pytorch) (9.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/venv/lib/python3.9/site-packages (from transformers->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.15.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/venv/lib/python3.9/site-packages (from transformers->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/venv/lib/python3.9/site-packages (from transformers->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.3.1)\n",
      "Requirement already satisfied: fsspec in /home/venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2023.6.0)\n",
      "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch)\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: portalocker in /home/venv/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.7.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/venv/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.9.0)\n",
      "Requirement already satisfied: colorama in /home/venv/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.4.6)\n",
      "Requirement already satisfied: lxml in /home/venv/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.9.2)\n",
      "Requirement already satisfied: pycparser in /home/venv/lib/python3.9/site-packages (from cffi->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/venv/lib/python3.9/site-packages (from requests->torchvision->x-clip->musiclm-pytorch) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/venv/lib/python3.9/site-packages (from requests->torchvision->x-clip->musiclm-pytorch) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/venv/lib/python3.9/site-packages (from requests->torchvision->x-clip->musiclm-pytorch) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/venv/lib/python3.9/site-packages (from requests->torchvision->x-clip->musiclm-pytorch) (2023.5.7)\n",
      "Building wheels for collected packages: encodec, fairseq, antlr4-python3-runtime\n",
      "  Building wheel for encodec (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45760 sha256=eff37c0bc13649c88d7c9187079e0af6ad6bc4c13953d0d45a0b47722f337cab\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/1d/9d/20/489d6aafffb505e18fcfcfbe722562f91c26af0a8a6da7d00b\n",
      "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairseq: filename=fairseq-0.12.2-cp39-cp39-linux_x86_64.whl size=11180490 sha256=272debae118a20e727a2fc9dbf37431687869b49a27eab4451a51031fe94fe27\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/59/35/87/2baf2e4ad37c83fd698c486b3d39f0e7022226fa52ab469c31\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=5d0cd425e0b8df5ec38dc75bdf5c367ad4bf925f324e2d9983d3acc5d44f06f0\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/42/3c/ae/14db087e6018de74810afe32eb6ac890ef9c68ba19b00db97a\n",
      "Successfully built encodec fairseq antlr4-python3-runtime\n",
      "Installing collected packages: sentencepiece, antlr4-python3-runtime, omegaconf, ftfy, einops, beartype, hydra-core, vector-quantize-pytorch, local-attention, lion-pytorch, fairseq, encodec, ema-pytorch, accelerate, x-clip, audiolm-pytorch, musiclm-pytorch\n",
      "  Attempting uninstall: antlr4-python3-runtime\n",
      "    Found existing installation: antlr4-python3-runtime 4.9.3\n",
      "    Uninstalling antlr4-python3-runtime-4.9.3:\n",
      "      Successfully uninstalled antlr4-python3-runtime-4.9.3\n",
      "  Attempting uninstall: omegaconf\n",
      "    Found existing installation: omegaconf 2.3.0\n",
      "    Uninstalling omegaconf-2.3.0:\n",
      "      Successfully uninstalled omegaconf-2.3.0\n",
      "  Attempting uninstall: hydra-core\n",
      "    Found existing installation: hydra-core 1.3.2\n",
      "    Uninstalling hydra-core-1.3.2:\n",
      "      Successfully uninstalled hydra-core-1.3.2\n",
      "Successfully installed accelerate-0.21.0 antlr4-python3-runtime-4.8 audiolm-pytorch-1.2.15 beartype-0.14.1 einops-0.6.1 ema-pytorch-0.2.3 encodec-0.1.1 fairseq-0.12.2 ftfy-6.1.1 hydra-core-1.0.7 lion-pytorch-0.1.2 local-attention-1.8.6 musiclm-pytorch-0.2.2 omegaconf-2.0.6 sentencepiece-0.1.99 vector-quantize-pytorch-1.6.30 x-clip-0.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install musiclm-pytorch # the dependant audiolm_pytorch needs to be 1.2.15 not the latest 1.2.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915e29f8-6b98-4c64-85b2-411ea7aad77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-07-18 06:19:33.795443: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-18 06:19:35.017192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-07-18 06:19:36 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spectrogram yielded shape of (65, 86), but had to be cropped to (64, 80) to be patchified for transformer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from musiclm_pytorch import MuLaN, AudioSpectrogramTransformer, TextTransformer\n",
    "\n",
    "audio_transformer = AudioSpectrogramTransformer(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    dim_head = 64,\n",
    "    spec_n_fft = 128,\n",
    "    spec_win_length = 24,\n",
    "    spec_aug_stretch_factor = 0.8\n",
    ")\n",
    "\n",
    "text_transformer = TextTransformer(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    dim_head = 64\n",
    ")\n",
    "\n",
    "mulan = MuLaN(\n",
    "    audio_transformer = audio_transformer,\n",
    "    text_transformer = text_transformer\n",
    ")\n",
    "\n",
    "# get a ton of <sound, text> pairs and train\n",
    "\n",
    "wavs = torch.randn(2, 1024)\n",
    "texts = torch.randint(0, 20000, (2, 256))\n",
    "\n",
    "loss = mulan(wavs, texts)\n",
    "loss.backward()\n",
    "\n",
    "# after much training, you can embed sounds and text into a joint embedding space\n",
    "# for conditioning the audio LM\n",
    "\n",
    "embeds = mulan.get_audio_latents(wavs)  # during training\n",
    "\n",
    "embeds = mulan.get_text_latents(texts)  # during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c85424-5df7-42b1-8ef6-515178f39a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from musiclm_pytorch import MuLaNEmbedQuantizer\n",
    "\n",
    "# setup the quantizer with the namespaced conditioning embeddings, unique per quantizer as well as namespace (per transformer)\n",
    "\n",
    "quantizer = MuLaNEmbedQuantizer(\n",
    "    mulan = mulan,                          # pass in trained mulan from above\n",
    "    conditioning_dims = (1024, 1024, 1024), # say all three transformers have model dimensions of 1024\n",
    "    namespaces = ('semantic', 'coarse', 'fine')\n",
    ")\n",
    "\n",
    "# now say you want the conditioning embeddings for semantic transformer\n",
    "\n",
    "wavs = torch.randn(2, 1024)\n",
    "conds = quantizer(wavs = wavs, namespace = 'semantic') # (2, 8, 1024) - 8 is number of quantizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab38a124-92c0-4457-862d-11b4e4a8f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import math\n",
    "import wave\n",
    "import struct\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from audiolm_pytorch import SoundStream, SoundStreamTrainer, HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer, HubertWithKmeans, CoarseTransformer, CoarseTransformerWrapper, CoarseTransformerTrainer, FineTransformer, FineTransformerWrapper, FineTransformerTrainer, AudioLM\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "# define all dataset paths, checkpoints, etc\n",
    "dataset_folder = \"placeholder_dataset\"\n",
    "soundstream_ckpt = \"results/soundstream.8.pt\" # this can change depending on number of steps\n",
    "hubert_ckpt = 'hubert/hubert_base_ls960.pt'\n",
    "hubert_quantizer = f'hubert/hubert_base_ls960_L9_km500.bin' # listed in row \"HuBERT Base (~95M params)\", column Quantizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b5232-5d11-40f7-9258-d9092fde1bfe",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a8b6776-cad2-41ea-8a14-19bbc7f315a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder data generation\n",
    "def get_sinewave(freq=440.0, duration_ms=200, volume=1.0, sample_rate=44100.0):\n",
    "  # code adapted from https://stackoverflow.com/a/33913403\n",
    "  audio = []\n",
    "  num_samples = duration_ms * (sample_rate / 1000.0)\n",
    "  for x in range(int(num_samples)):\n",
    "    audio.append(volume * math.sin(2 * math.pi * freq * (x / sample_rate)))\n",
    "  return audio\n",
    "\n",
    "def save_wav(file_name, audio, sample_rate=44100.0):\n",
    "  # Open up a wav file\n",
    "  wav_file=wave.open(file_name,\"w\")\n",
    "  # wav params\n",
    "  nchannels = 1\n",
    "  sampwidth = 2\n",
    "  # 44100 is the industry standard sample rate - CD quality.  If you need to\n",
    "  # save on file size you can adjust it downwards. The stanard for low quality\n",
    "  # is 8000 or 8kHz.\n",
    "  nframes = len(audio)\n",
    "  comptype = \"NONE\"\n",
    "  compname = \"not compressed\"\n",
    "  wav_file.setparams((nchannels, sampwidth, sample_rate, nframes, comptype, compname))\n",
    "  # WAV files here are using short, 16 bit, signed integers for the \n",
    "  # sample size.  So we multiply the floating point data we have by 32767, the\n",
    "  # maximum value for a short integer.  NOTE: It is theortically possible to\n",
    "  # use the floating point -1.0 to 1.0 data directly in a WAV file but not\n",
    "  # obvious how to do that using the wave module in python.\n",
    "  for sample in audio:\n",
    "      wav_file.writeframes(struct.pack('h', int( sample * 32767.0 )))\n",
    "  wav_file.close()\n",
    "  return\n",
    "\n",
    "def make_placeholder_dataset():\n",
    "  # Make a placeholder dataset with a few .wav files that you can \"train\" on, just to verify things work e2e\n",
    "  if os.path.isdir(dataset_folder):\n",
    "    return\n",
    "  os.makedirs(dataset_folder)\n",
    "  save_wav(f\"{dataset_folder}/example.wav\", get_sinewave())\n",
    "  save_wav(f\"{dataset_folder}/example2.wav\", get_sinewave(duration_ms=500))\n",
    "  os.makedirs(f\"{dataset_folder}/subdirectory\")\n",
    "  save_wav(f\"{dataset_folder}/subdirectory/example.wav\", get_sinewave(freq=330.0))\n",
    "\n",
    "make_placeholder_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0250cf23-8710-4d15-bb5b-e1a225dd4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual dataset. Uncomment this if you want to try training on real data\n",
    "\n",
    "# full dataset: https://www.openslr.org/12\n",
    "# We'll use https://us.openslr.org/resources/12/dev-clean.tar.gz development set, \"clean\" speech.\n",
    "# We *should* train on, well, training, but this is just to demo running things end-to-end at all so I just picked a small clean set.\n",
    "\n",
    "url = \"https://us.openslr.org/resources/12/dev-clean.tar.gz\"\n",
    "filename = \"dev-clean\"\n",
    "filename_targz = filename + \".tar.gz\"\n",
    "if not os.path.isfile(filename_targz):\n",
    "  urllib.request.urlretrieve(url, filename_targz)\n",
    "if not os.path.isdir(filename):\n",
    "  # open file\n",
    "  with tarfile.open(filename_targz) as t:\n",
    "    t.extractall(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0b4fb-82da-4112-a2ad-b1281bba7107",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Note: do NOT type \"y\" to overwrite previous experiments/ checkpoints when running through the cells here unless you're ready to the entire results folder! Otherwise you will end up erasing things (e.g. you train SoundStream first, and if you choose \"overwrite\" then you lose the SoundStream checkpoint when you then train SemanticTransformer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7243ba-dca5-4f25-bd71-4f6b94713e57",
   "metadata": {},
   "source": [
    "### SoundStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f91f1ba-145d-46bc-a834-dd2c8987e273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 2 samples and validating with randomly splitted 1 samples\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "do you want to clear previous experiment checkpoints and results? (y/n)  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: soundstream total loss: 84.767, soundstream recon loss: 0.295 | discr (scale 1) loss: 2.000 | discr (scale 0.5) loss: 2.000 | discr (scale 0.25) loss: 1.999\n",
      "0: saving to results\n",
      "0: saving model to results\n",
      "1: soundstream total loss: 79.417, soundstream recon loss: 0.279 | discr (scale 1) loss: 1.988 | discr (scale 0.5) loss: 1.994 | discr (scale 0.25) loss: 1.990\n",
      "2: soundstream total loss: 80.301, soundstream recon loss: 0.274 | discr (scale 1) loss: 1.972 | discr (scale 0.5) loss: 1.986 | discr (scale 0.25) loss: 1.979\n",
      "2: saving to results\n",
      "3: soundstream total loss: 85.131, soundstream recon loss: 0.277 | discr (scale 1) loss: 1.949 | discr (scale 0.5) loss: 1.973 | discr (scale 0.25) loss: 1.964\n",
      "4: soundstream total loss: 84.770, soundstream recon loss: 0.280 | discr (scale 1) loss: 1.917 | discr (scale 0.5) loss: 1.956 | discr (scale 0.25) loss: 1.943\n",
      "4: saving to results\n",
      "4: saving model to results\n",
      "5: soundstream total loss: 84.602, soundstream recon loss: 0.279 | discr (scale 1) loss: 1.875 | discr (scale 0.5) loss: 1.935 | discr (scale 0.25) loss: 1.916\n",
      "6: soundstream total loss: 83.251, soundstream recon loss: 0.277 | discr (scale 1) loss: 1.822 | discr (scale 0.5) loss: 1.907 | discr (scale 0.25) loss: 1.882\n",
      "6: saving to results\n",
      "7: soundstream total loss: 82.473, soundstream recon loss: 0.274 | discr (scale 1) loss: 1.760 | discr (scale 0.5) loss: 1.873 | discr (scale 0.25) loss: 1.839\n",
      "8: soundstream total loss: 80.802, soundstream recon loss: 0.273 | discr (scale 1) loss: 1.687 | discr (scale 0.5) loss: 1.832 | discr (scale 0.25) loss: 1.787\n",
      "8: saving to results\n",
      "8: saving model to results\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "\n",
    "trainer = SoundStreamTrainer(\n",
    "    soundstream,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 4,\n",
    "    grad_accum_every = 8,         # effective batch size of 32\n",
    "    data_max_length = 320 * 32,\n",
    "    save_results_every = 2,\n",
    "    save_model_every = 4,\n",
    "    num_train_steps = 9\n",
    ")# .cuda()\n",
    "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
    "# adjusting save_*_every variables for the same reason\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfd9e73-ed04-4eba-b147-8f38a01d8a57",
   "metadata": {},
   "source": [
    "### SemanticTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6263f875-8b1e-4474-9852-b04e65d88d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 2567 samples and validating with randomly splitted 136 samples\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "do you want to clear previous experiment checkpoints and results? (y/n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss: 6.3929524421691895\n",
      "0: valid loss 6.680841445922852\n",
      "0: saving model to results\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from audiolm_pytorch import HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer\n",
    "\n",
    "# hubert checkpoints can be downloaded at\n",
    "# https://github.com/facebookresearch/fairseq/tree/main/examples/hubert\n",
    "if not os.path.isdir(\"hubert\"):\n",
    "  os.makedirs(\"hubert\")\n",
    "if not os.path.isfile(hubert_ckpt):\n",
    "  hubert_ckpt_download = f\"https://dl.fbaipublicfiles.com/{hubert_ckpt}\"\n",
    "  urllib.request.urlretrieve(hubert_ckpt_download, f\"./{hubert_ckpt}\")\n",
    "if not os.path.isfile(hubert_quantizer):\n",
    "  hubert_quantizer_download = f\"https://dl.fbaipublicfiles.com/{hubert_quantizer}\"\n",
    "  urllib.request.urlretrieve(hubert_quantizer_download, f\"./{hubert_quantizer}\")\n",
    "\n",
    "\n",
    "wav2vec = HubertWithKmeans(\n",
    "    checkpoint_path = './hubert/hubert_base_ls960.pt',\n",
    "    kmeans_path = './hubert/hubert_base_ls960_L9_km500.bin'\n",
    ")\n",
    "\n",
    "semantic_transformer = SemanticTransformer(\n",
    "    num_semantic_tokens = wav2vec.codebook_size,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    audio_text_condition = True      # this must be set to True (same for CoarseTransformer and FineTransformers)\n",
    ")# .cuda()\n",
    "\n",
    "trainer = SemanticTransformerTrainer(\n",
    "    transformer = semantic_transformer,\n",
    "    wav2vec = wav2vec,\n",
    "    audio_conditioner = quantizer,   # pass in the MulanEmbedQuantizer instance above\n",
    "    folder = './dev-clean', # dataset_folder, #'/path/to/audio/files',\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    num_train_steps = 1\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "826faf40-d3ee-4437-b57b-2179d728f828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==0.24.0\n",
      "  Downloading scikit_learn-0.24.0-cp39-cp39-manylinux2010_x86_64.whl (23.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /home/venv/lib/python3.9/site-packages (from scikit-learn==0.24.0) (1.23.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/venv/lib/python3.9/site-packages (from scikit-learn==0.24.0) (1.10.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/venv/lib/python3.9/site-packages (from scikit-learn==0.24.0) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/venv/lib/python3.9/site-packages (from scikit-learn==0.24.0) (3.1.0)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.3.0\n",
      "    Uninstalling scikit-learn-1.3.0:\n",
      "      Successfully uninstalled scikit-learn-1.3.0\n",
      "Successfully installed scikit-learn-0.24.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn==0.24.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02176777-d479-45a7-b26c-5090d1b81f81",
   "metadata": {},
   "source": [
    "### CoarseTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18a67e22-3e0d-4081-8932-85fe7bfa78f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 2 samples and validating with randomly splitted 1 samples\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "do you want to clear previous experiment checkpoints and results? (y/n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss: 62.850337982177734\n",
      "0: valid loss 23.167505264282227\n",
      "0: saving model to results\n",
      "1: loss: 21.782703399658203\n",
      "2: loss: 18.83949089050293\n",
      "2: valid loss 16.794471740722656\n",
      "3: loss: 18.255786895751953\n",
      "4: loss: 13.990899085998535\n",
      "4: valid loss 13.368377685546875\n",
      "4: saving model to results\n",
      "5: loss: 16.393291473388672\n",
      "6: loss: 14.13045883178711\n",
      "6: valid loss 10.995705604553223\n",
      "7: loss: 15.738718032836914\n",
      "8: loss: 10.971385955810547\n",
      "8: valid loss 9.304496765136719\n",
      "8: saving model to results\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "wav2vec = HubertWithKmeans(\n",
    "    checkpoint_path = f'./{hubert_ckpt}',\n",
    "    kmeans_path = f'./{hubert_quantizer}'\n",
    ")\n",
    "\n",
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "\n",
    "soundstream.load(f\"./{soundstream_ckpt}\")\n",
    "\n",
    "coarse_transformer = CoarseTransformer(\n",
    "    num_semantic_tokens = wav2vec.codebook_size,\n",
    "    codebook_size = 1024,\n",
    "    num_coarse_quantizers = 3,\n",
    "    dim = 512,\n",
    "    depth = 6\n",
    ")\n",
    "\n",
    "trainer = CoarseTransformerTrainer(\n",
    "    transformer = coarse_transformer,\n",
    "    codec = soundstream,\n",
    "    wav2vec = wav2vec,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    save_results_every = 2,\n",
    "    save_model_every = 4,\n",
    "    num_train_steps = 9\n",
    ")\n",
    "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
    "# adjusting save_*_every variables for the same reason\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cbfb7f-873a-4c84-a024-cac6ddac5ac0",
   "metadata": {},
   "source": [
    "### FineTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc2acdbb-f909-434b-801b-e0df8b959497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 2 samples and validating with randomly splitted 1 samples\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "do you want to clear previous experiment checkpoints and results? (y/n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss: 72.35494995117188\n",
      "0: valid loss 9.221342086791992\n",
      "0: saving model to results\n",
      "1: loss: 10.500727653503418\n",
      "2: loss: 8.58434009552002\n",
      "3: loss: 6.5776567459106445\n",
      "4: loss: 6.666227340698242\n",
      "5: loss: 7.072376251220703\n",
      "6: loss: 6.305515289306641\n",
      "7: loss: 4.945314884185791\n",
      "8: loss: 5.616884708404541\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "\n",
    "soundstream.load(f\"./{soundstream_ckpt}\")\n",
    "\n",
    "fine_transformer = FineTransformer(\n",
    "    num_coarse_quantizers = 3,\n",
    "    num_fine_quantizers = 5,\n",
    "    codebook_size = 1024,\n",
    "    dim = 512,\n",
    "    depth = 6\n",
    ")\n",
    "\n",
    "trainer = FineTransformerTrainer(\n",
    "    transformer = fine_transformer,\n",
    "    codec = soundstream,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    num_train_steps = 9\n",
    ")\n",
    "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
    "# adjusting save_*_every variables for the same reason\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c954a-bf4d-47a9-8ff6-8348f0351367",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23f30a4e-f287-4609-ae35-92b225be7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything together\n",
    "audiolm = AudioLM(\n",
    "    wav2vec = wav2vec,\n",
    "    codec = soundstream,\n",
    "    semantic_transformer = semantic_transformer,\n",
    "    coarse_transformer = coarse_transformer,\n",
    "    fine_transformer = fine_transformer\n",
    ")\n",
    "\n",
    "# generated_wav = audiolm(batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32da03b4-cc13-4032-a7d1-1c0185560259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating semantic:   1%|          | 20/2048 [00:04<07:02,  4.80it/s]\n",
      "generating coarse: 100%|██████████| 512/512 [20:51<00:00,  2.45s/it]\n",
      "generating fine: 100%|██████████| 512/512 [5:58:24<00:00, 42.00s/it]  \n"
     ]
    }
   ],
   "source": [
    "# you need the trained AudioLM (audio_lm) from above\n",
    "# with the MulanEmbedQuantizer (mulan_embed_quantizer)\n",
    "\n",
    "from musiclm_pytorch import MusicLM\n",
    "\n",
    "musiclm = MusicLM(\n",
    "    audio_lm = audiolm,                 # `AudioLM` from https://github.com/lucidrains/audiolm-pytorch\n",
    "    mulan_embed_quantizer = quantizer    # the `MuLaNEmbedQuantizer` from above\n",
    ")\n",
    "\n",
    "music = musiclm('the crystalline sounds of the piano in a ballroom', num_samples = 1) # sample 4 and pick the top match with mulan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbd8ad05-f83d-445d-a535-7f8ce180af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"piano.wav\"\n",
    "sample_rate = 44100\n",
    "torchaudio.save(output_path, music.cpu(), sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf3d07-fdfc-48a6-a7fa-8a1210cecfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio as IPythonAudio\n",
    "\n",
    "IPythonAudio(\"piano.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
