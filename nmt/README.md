# Transformer (NMT)

The Transformer, introduced in the paper Attention Is All You Need, is a powerful sequence-to-sequence modeling architecture capable of producing state-of-the-art neural machine translation (NMT) systems.

Recently, the fairseq team has explored large-scale semi-supervised training of Transformers using back-translated data, further improving translation quality over the original model. 

Please refer to

- NMT.ipynb

It requires 4 CPUs/12G RAM/1 GPU.
